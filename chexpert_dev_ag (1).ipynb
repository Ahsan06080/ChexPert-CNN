{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRpTRxP6vyfZ",
    "outputId": "0b7f4052-02dd-446e-f62d-3fa74bf71a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b01RpZPh2W46",
    "outputId": "2722aff6-5976-41be-89ef-216e35439cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.4.1\n",
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n",
      "Building wheels for collected packages: efficientnet-pytorch\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=21787c76d4c118da86f5b522e285af64f139f147121c00cc2dac025a887fc762\n",
      "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
      "Successfully built efficientnet-pytorch\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX\n",
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhAlT_tnv5aO",
    "outputId": "6d680759-15e6-4e8d-e5b9-918728a2d94d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "shutil.copyfile('/content/drive/MyDrive/Chexpert.zip' , 'Chexpert.zip')\n",
    "!unzip Chexpert.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttx3fjHnwe9d",
    "outputId": "dd625e78-b86d-4bdd-a0ad-3fd267b66876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Chexpert\n"
     ]
    }
   ],
   "source": [
    "%cd Chexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JqAHN3YQv687"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "import cv2\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from PIL import Image\n",
    "from data.imgaug import GetTransforms\n",
    "from data.utils import transform\n",
    "np.random.seed(0)\n",
    "from easydict import EasyDict as edict\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "from shutil import copyfile\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adadelta, Adagrad, Adam, RMSprop\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrpMauEEvyfc",
    "tags": []
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4SeBSgnHvyfd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sTZ_FbGzvyfd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0Hi2tX_tvyfe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qNDeuli0vyfe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jJvn0g6wvyfe"
   },
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((320, 320)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "data_aug_transforms = transforms.Compose([\n",
    "        transforms.Resize((320, 320)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=(-180,180)),\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 2)),#new\n",
    "        transforms.RandomAutocontrast(p=0.1),\n",
    "\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "class CheXDataset(Dataset):\n",
    "    def __init__(self, csv_path, diseases,transform=None, make_one=1, mode='train'):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        self.df = self.df.fillna(0)\n",
    "        self.make_one = make_one\n",
    "        header = list(df.columns)\n",
    "        self._label_header = [\n",
    "                header[7],\n",
    "                header[10],\n",
    "                header[11],\n",
    "                header[13],\n",
    "                header[15]]\n",
    "        self.defult_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.df['Path'].values[idx]).convert('RGB') \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else :\n",
    "            image = self.defult_transform(image)\n",
    "            \n",
    "  \n",
    "        labels = np.zeros(len(diseases))\n",
    "        \n",
    "        if int(self.df['No Finding'].values[idx]) == 1  :\n",
    "            labels = labels\n",
    "        else:\n",
    "            for i in range(len(labels)):\n",
    "                if self.df[diseases[i]].values[idx] != -1 :\n",
    "                    labels[i] = self.df[diseases[i]].values[idx]\n",
    "                elif self.make_one :\n",
    "                    labels[i] = 1\n",
    "                else :\n",
    "                    labels[i] = 0  \n",
    "                \n",
    "                \n",
    "        return image,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "u9r-IoFCvyff"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('config/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JdBuVBscvyfg"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('config/train.csv')\n",
    "# csv_path = 'config/train.csv'\n",
    "# diseases = ['Cardiomegaly', 'Edema', 'Consolidation', 'Atelectasis', 'Pleural_Effusion']\n",
    "# dataset = CheXDataset(csv_path, diseases, data_aug_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ULRGjgGivyfg"
   },
   "outputs": [],
   "source": [
    "# a,b = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BXVn58zrvyfh"
   },
   "outputs": [],
   "source": [
    "# b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "E55YfZtqvyfi"
   },
   "outputs": [],
   "source": [
    "diseases = ['Cardiomegaly', 'Edema', 'Consolidation', 'Atelectasis', 'Pleural_Effusion']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYVohA57vyfi"
   },
   "source": [
    "# Model|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PVVYh-xnZuuC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PcamPool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PcamPool, self).__init__()\n",
    "\n",
    "    def forward(self, feat_map, logit_map):\n",
    "        assert logit_map is not None\n",
    "\n",
    "        prob_map = torch.sigmoid(logit_map)\n",
    "        weight_map = prob_map / prob_map.sum(dim=2, keepdim=True)\\\n",
    "            .sum(dim=3, keepdim=True)\n",
    "        feat = (feat_map * weight_map).sum(dim=2, keepdim=True)\\\n",
    "            .sum(dim=3, keepdim=True)\n",
    "\n",
    "        return feat\n",
    "\n",
    "\n",
    "class LogSumExpPool(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=.9):\n",
    "        super(LogSumExpPool, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, feat_map):\n",
    "        \"\"\"\n",
    "        Numerically stable implementation of the operation\n",
    "        Arguments:\n",
    "            feat_map(Tensor): tensor with shape (N, C, H, W)\n",
    "            return(Tensor): tensor with shape (N, C, 1, 1)\n",
    "        \"\"\"\n",
    "        (N, C, H, W) = feat_map.shape\n",
    "\n",
    "        # (N, C, 1, 1) m\n",
    "        m, _ = torch.max(\n",
    "            feat_map, dim=-1, keepdim=True)[0].max(dim=-2, keepdim=True)\n",
    "\n",
    "        # (N, C, H, W) value0\n",
    "        value0 = feat_map - m\n",
    "        area = 1.0 / (H * W)\n",
    "        g = self.gamma\n",
    "\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        return m + 1 / g * torch.log(area * torch.sum(\n",
    "            torch.exp(g * value0), dim=(-1, -2), keepdim=True))\n",
    "\n",
    "\n",
    "class ExpPool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ExpPool, self).__init__()\n",
    "\n",
    "    def forward(self, feat_map):\n",
    "        \"\"\"\n",
    "        Numerically stable implementation of the operation\n",
    "        Arguments:\n",
    "            feat_map(Tensor): tensor with shape (N, C, H, W)\n",
    "            return(Tensor): tensor with shape (N, C, 1, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        EPSILON = 1e-7\n",
    "        (N, C, H, W) = feat_map.shape\n",
    "        m, _ = torch.max(\n",
    "            feat_map, dim=-1, keepdim=True)[0].max(dim=-2, keepdim=True)\n",
    "\n",
    "        # caculate the sum of exp(xi)\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        sum_exp = torch.sum(torch.exp(feat_map - m),\n",
    "                            dim=(-1, -2), keepdim=True)\n",
    "\n",
    "        # prevent from dividing by zero\n",
    "        sum_exp += EPSILON\n",
    "\n",
    "        # caculate softmax in shape of (H,W)\n",
    "        exp_weight = torch.exp(feat_map - m) / sum_exp\n",
    "        weighted_value = feat_map * exp_weight\n",
    "\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        return torch.sum(weighted_value, dim=(-1, -2), keepdim=True)\n",
    "\n",
    "\n",
    "class LinearPool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearPool, self).__init__()\n",
    "\n",
    "    def forward(self, feat_map):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            feat_map(Tensor): tensor with shape (N, C, H, W)\n",
    "            return(Tensor): tensor with shape (N, C, 1, 1)\n",
    "        \"\"\"\n",
    "        EPSILON = 1e-7\n",
    "        (N, C, H, W) = feat_map.shape\n",
    "\n",
    "        # sum feat_map's last two dimention into a scalar\n",
    "        # so the shape of sum_input is (N,C,1,1)\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        sum_input = torch.sum(feat_map, dim=(-1, -2), keepdim=True)\n",
    "\n",
    "        # prevent from dividing by zero\n",
    "        sum_input += EPSILON\n",
    "\n",
    "        # caculate softmax in shape of (H,W)\n",
    "        linear_weight = feat_map / sum_input\n",
    "        weighted_value = feat_map * linear_weight\n",
    "\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        return torch.sum(weighted_value, dim=(-1, -2), keepdim=True)\n",
    "\n",
    "\n",
    "class GlobalPool(nn.Module):\n",
    "\n",
    "    def __init__(self, pool = 'AVG'):\n",
    "        super(GlobalPool, self).__init__()\n",
    "        self.pool = pool\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.exp_pool = ExpPool()\n",
    "        self.pcampool = PcamPool()\n",
    "        self.linear_pool = LinearPool()\n",
    "        self.lse_pool = LogSumExpPool()\n",
    "\n",
    "    def cuda(self, device=None):\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "\n",
    "    def forward(self, feat_map, logit_map):\n",
    "        if self.pool == 'AVG':\n",
    "            return self.avgpool(feat_map)\n",
    "        elif self.pool == 'MAX':\n",
    "            return self.maxpool(feat_map)\n",
    "        elif self.pool == 'PCAM':\n",
    "            return self.pcampool(feat_map, logit_map)\n",
    "        elif self.pool == 'AVG_MAX':\n",
    "            a = self.avgpool(feat_map)\n",
    "            b = self.maxpool(feat_map)\n",
    "            return torch.cat((a, b), 1)\n",
    "        elif self.pool == 'AVG_MAX_LSE':\n",
    "            a = self.avgpool(feat_map)\n",
    "            b = self.maxpool(feat_map)\n",
    "            c = self.lse_pool(feat_map)\n",
    "            return torch.cat((a, b, c), 1)\n",
    "        elif self.pool == 'EXP':\n",
    "            return self.exp_pool(feat_map)\n",
    "        elif self.pool == 'LINEAR':\n",
    "            return pool(feat_map)\n",
    "        elif self.pool == 'LSE':\n",
    "            return self.lse_pool(feat_map)\n",
    "        else:\n",
    "            raise Exception('Unknown pooling type : {}'\n",
    "                            .format(self.cfg.global_pool))\n",
    "class CAModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Re-implementation of Squeeze-and-Excitation (SE) block described in:\n",
    "    *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n",
    "    code reference:\n",
    "    https://github.com/kobiso/CBAM-keras/blob/master/models/attention_module.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels, reduc_ratio=2):\n",
    "        super(CAModule, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.reduc_ratio = reduc_ratio\n",
    "\n",
    "        self.fc1 = nn.Linear(num_channels, num_channels // reduc_ratio,\n",
    "                             bias=True)\n",
    "        self.fc2 = nn.Linear(num_channels // reduc_ratio, num_channels,\n",
    "                             bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, feat_map):\n",
    "        # attention branch--squeeze operation\n",
    "        gap_out = feat_map.view(feat_map.size()[0], self.num_channels,\n",
    "                                -1).mean(dim=2)\n",
    "\n",
    "        # attention branch--excitation operation\n",
    "        fc1_out = self.relu(self.fc1(gap_out))\n",
    "        fc2_out = self.sigmoid(self.fc2(fc1_out))\n",
    "\n",
    "        # attention operation\n",
    "        fc2_out = fc2_out.view(fc2_out.size()[0], fc2_out.size()[1], 1, 1)\n",
    "        feat_map = torch.mul(feat_map, fc2_out)\n",
    "\n",
    "        return feat_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UyHxEOg5vyfi"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov 11 23:26:04 2020\n",
    "\n",
    "@author: Ahsan\n",
    "\"\"\"\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from collections import OrderedDict\n",
    "import efficientnet_pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "__all__ = ['EfficientNet', 'EfficientNet_AG']\n",
    "\n",
    "class Classifier_Pool(nn.Module) :\n",
    "    def __init__(self, n_class) :\n",
    "        super(Classifier_Pool,self).__init__()\n",
    "        self.n_class = n_class\n",
    "        for i in range(self.n_class):\n",
    "            setattr(self,'conv_'+str(i+1), nn.Conv2d(1536,1, kernel_size=1,stride = 1, padding = 0))\n",
    "            \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.global_pool = GlobalPool(pool = 'AVG')\n",
    "        for i in range(self.n_class):\n",
    "            setattr(self,'attention_'+str(i+1), CAModule(1536))\n",
    "        \n",
    "    def forward(self,feat_map):\n",
    "        logits = list()\n",
    "        logit_maps = list()\n",
    "        #attentions = list()\n",
    "        # print(feat_map.shape)\n",
    "        for i in range(self.n_class) :\n",
    "        \n",
    "            classifier = getattr(self, 'conv_' + str(i+1))\n",
    "            attention = getattr(self, 'attention_'+str(i+1))\n",
    "            feat_map = attention(feat_map)\n",
    "            logit_map = None\n",
    "            logit_map = classifier(feat_map)\n",
    "            logit_maps.append(logit_map.squeeze())\n",
    "            feat = self.global_pool(feat_map, logit_map)\n",
    "            logit = classifier(feat)\n",
    "            logit = logit.squeeze(-1).squeeze(-1)\n",
    "            #feat = F.dropout(feat, p=self.fc_drop, training=self.training)\n",
    "            logit = self.sigmoid(logit) \n",
    "            logits.append(logit)\n",
    "            \n",
    "        if logits[0].shape[0] == 1 :\n",
    "            output = torch.cat(logits,dim = 0).t() \n",
    "        else :\n",
    "            output = torch.cat(logits,dim = 1)                        \n",
    "        return output,logit_maps\n",
    "def my_efficientnet_forward(self, inputs):\n",
    "    \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
    "    bs = inputs.size(0)\n",
    "    \n",
    "    # Convolution layers\n",
    "    x = self.extract_features(inputs)\n",
    "    \n",
    "    # Pooling and final linear layer\n",
    "    #print(x.shape)\n",
    "    \n",
    "    return x\n",
    "\n",
    "efficientnet_pytorch.EfficientNet.forward.__code__ = my_efficientnet_forward.__code__\n",
    "\n",
    "def EfficientNet_AG(compound_coeff=0, num_classes= 1000, **kwargs):\n",
    "   \n",
    "    model = EfficientNet_(num_init_features=64, compound_coeff=compound_coeff, num_classes=num_classes )\n",
    "\n",
    "    return model\n",
    "\n",
    "class EfficientNet_(nn.Module) :\n",
    "    \n",
    "    def __init__(self,num_init_features=3, compound_coeff = 0, weights_path=None, num_classes=5) :\n",
    "        super(EfficientNet_, self).__init__()\n",
    "        \n",
    "        model= EfficientNet.from_pretrained(f'efficientnet-b{compound_coeff}',in_channels=3)\n",
    "        \n",
    "        self.model = model\n",
    "        self.model.add_module('norm5', nn.BatchNorm2d(1536))\n",
    "        self.classifier = Classifier_Pool(n_class=5)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "       \n",
    "    \n",
    "    def forward(self, x):\n",
    "#        features = self.features(x)\n",
    "#        print(features.shape)\n",
    "        features = self.model(x)\n",
    "#        features = self.bn(features) \n",
    "        #print(features.shape)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        #out_after_pooling = F.avg_pool2d(out, kernel_size=7, stride=7).view(features.size(0), -1)\n",
    "        #print(out_after_pooling.shape)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        #print(out.shape)                   \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b69nW0X8vyfj",
    "outputId": "b5b4c713-cf2c-4798-9b75-f530b5f75bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((1,3,320,320))\n",
    "model = EfficientNet_AG(3,5)\n",
    "b = model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMfJ906tvyfj",
    "outputId": "4b51a3bc-30cb-42e1-f9f3-da89aa7d13f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = b\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "IynnOQJcvyfj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "s8XUfvw3vyfk"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "iuMzVv-uvyfk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mHyactsvyfk",
    "tags": []
   },
   "source": [
    "# Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "y90iKMRKvyfk"
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "cfg = {\n",
    "     \"train_csv\": \"config/train.csv\",\n",
    "    \"dev_csv\": \"config/dev.csv\",\n",
    "    \"backbone\": \"densenet121\",\n",
    "    \"width\": 320,\n",
    "    \"height\": 320,\n",
    "    \"long_side\": 320,\n",
    "    \"fix_ratio\": True,\n",
    "    \"pixel_mean\": 128.0,\n",
    "    \"pixel_std\": 64.0,\n",
    "    \"use_pixel_std\": True,\n",
    "    \"use_equalizeHist\": True,\n",
    "    \"use_transforms_type\": \"Aug\",\n",
    "    \"gaussian_blur\": 3,\n",
    "    \"border_pad\": \"pixel_mean\",\n",
    "    \"num_classes\": [1,1,1,1,1],\n",
    "    \"batch_weight\": True,\n",
    "    \"enhance_index\": [2,6],\n",
    "    \"enhance_times\": 1,\n",
    "    \"pos_weight\": [1,1,1,1,1],\n",
    "    \"train_batch_size\": 16,\n",
    "    \"dev_batch_size\": 1,\n",
    "    \"pretrained\": True,\n",
    "    \"log_every\": 50,\n",
    "    \"test_every\": 200,\n",
    "    \"epoch\": 30,\n",
    "    \"norm_type\": \"BatchNorm\",\n",
    "    \"global_pool\": \"PCAM\",\n",
    "    \"fc_bn\": False,\n",
    "    \"attention_map\": \"None\",\n",
    "    \"lse_gamma\": 0.5,\n",
    "    \"fc_drop\": 0,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"criterion\": \"BCE\",\n",
    "    \"lr\": 0.001,\n",
    "    \"lr_factor\": 0.1,\n",
    "    \"lr_epochs\": [2],\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"best_target\": \"auc\",\n",
    "    \"save_top_k\": 30,\n",
    "    \"save_index\": [0,1,2,3,4],\n",
    "    'save_path' : '/content/drive/MyDrive/Biomed/efficient net',\n",
    "    'logtofile' : True,\n",
    "    'resume' : False,\n",
    "    'pre_train' : None,\n",
    "    'verbose' :  False\n",
    "}\n",
    "cfg  = edict(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fgn8vA3qvyfl"
   },
   "source": [
    "## Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "mPSh0ObHvyfl"
   },
   "outputs": [],
   "source": [
    "# output = torch.rand((4,5)).ge(0.5)\n",
    "# label = torch.ones((4,5))\n",
    "# #label = torch.sigmoid(output[index].view(-1)).ge(0.5).float()\n",
    "# #acc = (target == label).float().sum() / len(label)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "_3e_d_ihvyfl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "G6pt0dF0vyfl"
   },
   "outputs": [],
   "source": [
    "def train_epoch(summary, summary_dev, cfg, model, dataloader,\n",
    "                dataloader_dev, optimizer, summary_writer, best_dict,\n",
    "                dev_header):\n",
    "    torch.set_grad_enabled(True)\n",
    "    model.train()\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    steps = len(dataloader_train)\n",
    "    dataiter = iter(dataloader_train)\n",
    "    num_tasks = 5\n",
    "    label_header = dataloader.dataset._label_header\n",
    "    #num_tasks = len(cfg.num_classes)\n",
    "\n",
    "    time_now = time.time()\n",
    "    loss_sum = np.zeros(num_tasks)\n",
    "    acc_sum = np.zeros(num_tasks)\n",
    "    for step in tqdm.tqdm(range(steps)):\n",
    "        image, target = next(dataiter)\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        output,_ = model(image)\n",
    "        # different number of tasks\n",
    "        optimizer.zero_grad()\n",
    "        loss_t = F.binary_cross_entropy(output.float(), target.float(),reduction = 'none').sum(dim = 0)\n",
    "        loss_sum += np.array([i.item() for i in loss_t])\n",
    "        loss =  F.binary_cross_entropy(output.float(), target.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        label = output.ge(0.5).float()\n",
    "        acc_t = (target == label).float().sum(dim=0) / (label.shape[0])\n",
    "        acc_sum += np.array([i.item() for i in acc_t])\n",
    "        summary['step'] += 1\n",
    "\n",
    "        if summary['step'] % cfg.log_every == 0:\n",
    "            time_spent = time.time() - time_now\n",
    "            time_now = time.time()\n",
    "\n",
    "            loss_sum /= cfg.log_every\n",
    "            acc_sum /= cfg.log_every\n",
    "            loss_str = ' '.join(map(lambda x: '{:.5f}'.format(x), loss_sum))\n",
    "            acc_str = ' '.join(map(lambda x: '{:.3f}'.format(x), acc_sum))\n",
    "\n",
    "            sys.stdout.write(\n",
    "                '{}, Train, Epoch : {}, Step : {}, Loss : {},Acc : {}, Run Time : {:.2f} sec'\n",
    "                .format(time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        summary['epoch'] + 1, summary['step'], loss_str,\n",
    "                        acc_str, time_spent))\n",
    "\n",
    "            for t in range(num_tasks):\n",
    "                summary_writer.add_scalar(\n",
    "                    'train/loss_{}'.format(label_header[t]), loss_sum[t],\n",
    "                    summary['step'])\n",
    "                summary_writer.add_scalar(\n",
    "                    'train/acc_{}'.format(label_header[t]), acc_sum[t],\n",
    "                    summary['step'])\n",
    "\n",
    "            loss_sum = np.zeros(num_tasks)\n",
    "            acc_sum = np.zeros(num_tasks)\n",
    "\n",
    "        if summary['step'] % cfg.test_every == 0:\n",
    "            time_now = time.time()\n",
    "            summary_dev, predlist, true_list = test_epoch(\n",
    "                summary_dev, cfg, model, dataloader_dev)\n",
    "            time_spent = time.time() - time_now\n",
    "\n",
    "            auclist = []\n",
    "            for i in range(len(cfg.num_classes)):\n",
    "                y_pred = predlist[i]\n",
    "                y_true = true_list[i]\n",
    "                fpr, tpr, thresholds = metrics.roc_curve(\n",
    "                    y_true, y_pred, pos_label=1)\n",
    "                auc = metrics.auc(fpr, tpr)\n",
    "                auclist.append(auc)\n",
    "            summary_dev['auc'] = np.array(auclist)\n",
    "\n",
    "            loss_dev_str = ' '.join(map(lambda x: '{:.5f}'.format(x),\n",
    "                                        summary_dev['loss']))\n",
    "            acc_dev_str = ' '.join(map(lambda x: '{:.3f}'.format(x),\n",
    "                                       summary_dev['acc']))\n",
    "            auc_dev_str = ' '.join(map(lambda x: '{:.3f}'.format(x),\n",
    "                                       summary_dev['auc']))\n",
    "\n",
    "            sys.stdout.write('{}, Dev, Step : {}, Loss : {}, Acc : {}, Auc : {}, Mean auc: {:.3f} Run Time : {:.2f} sec'.format(\n",
    "                    time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    summary['step'],\n",
    "                    loss_dev_str,\n",
    "                    acc_dev_str,\n",
    "                    auc_dev_str,\n",
    "                    summary_dev['auc'].mean(),\n",
    "                    time_spent))\n",
    "\n",
    "            for t in range(len(cfg.num_classes)):\n",
    "                summary_writer.add_scalar(\n",
    "                    'dev/loss_{}'.format(dev_header[t]),\n",
    "                    summary_dev['loss'][t], summary['step'])\n",
    "                summary_writer.add_scalar(\n",
    "                    'dev/acc_{}'.format(dev_header[t]), summary_dev['acc'][t],\n",
    "                    summary['step'])\n",
    "                summary_writer.add_scalar(\n",
    "                    'dev/auc_{}'.format(dev_header[t]), summary_dev['auc'][t],\n",
    "                    summary['step'])\n",
    "\n",
    "            save_best = False\n",
    "            mean_acc = summary_dev['acc'][cfg.save_index].mean()\n",
    "            if mean_acc >= best_dict['acc_dev_best']:\n",
    "                best_dict['acc_dev_best'] = mean_acc\n",
    "                if cfg.best_target == 'acc':\n",
    "                    save_best = True\n",
    "\n",
    "            mean_auc = summary_dev['auc'][cfg.save_index].mean()\n",
    "            if mean_auc >= best_dict['auc_dev_best']:\n",
    "                best_dict['auc_dev_best'] = mean_auc\n",
    "                if cfg.best_target == 'auc':\n",
    "                    save_best = True\n",
    "\n",
    "            mean_loss = summary_dev['loss'][cfg.save_index].mean()\n",
    "            if mean_loss <= best_dict['loss_dev_best']:\n",
    "                best_dict['loss_dev_best'] = mean_loss\n",
    "                if cfg.best_target == 'loss':\n",
    "                    save_best = True\n",
    "\n",
    "            if save_best:\n",
    "                torch.save(\n",
    "                    {'epoch': summary['epoch'],\n",
    "                     'step': summary['step'],\n",
    "                     'acc_dev_best': best_dict['acc_dev_best'],\n",
    "                     'auc_dev_best': best_dict['auc_dev_best'],\n",
    "                     'loss_dev_best': best_dict['loss_dev_best'],\n",
    "                     'state_dict': model.state_dict()},\n",
    "                    os.path.join(cfg.save_path, 'best{}.tar'.format(\n",
    "                        best_dict['best_idx']))\n",
    "                )\n",
    "                best_dict['best_idx'] += 1\n",
    "                if best_dict['best_idx'] > cfg.save_top_k:\n",
    "                    best_dict['best_idx'] = 1\n",
    "                logging.info(\n",
    "                    '{}, Best, Step : {}, Loss : {}, Acc : {},Auc :{},'\n",
    "                    'Best Auc : {:.3f}' .format(\n",
    "                        time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        summary['step'],\n",
    "                        loss_dev_str,\n",
    "                        acc_dev_str,\n",
    "                        auc_dev_str,\n",
    "                        best_dict['auc_dev_best']))\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "    summary['epoch'] += 1\n",
    "\n",
    "    return summary, best_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "k-VcnqXcvyfm"
   },
   "outputs": [],
   "source": [
    "def test_epoch(summary, cfg, model, dataloader):\n",
    "    torch.set_grad_enabled(False)\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    steps = len(dataloader)\n",
    "    dataiter = iter(dataloader)\n",
    "    num_tasks = len(cfg.num_classes)\n",
    "\n",
    "    loss_sum = np.zeros(num_tasks)\n",
    "    acc_sum = np.zeros(num_tasks)\n",
    "\n",
    "    predlist = list()\n",
    "    true_list = list()\n",
    "    for step in range(steps):\n",
    "        image, target = next(dataiter)\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "         \n",
    "        output,_ = model(image)\n",
    "        if step == 0: \n",
    "            output_t = output\n",
    "            target_t = target\n",
    "        else :\n",
    "            output_t = torch.cat([output_t, output],dim = 0)\n",
    "            target_t = torch.cat([target_t, target],dim = 0)\n",
    "        # different number of tasks\n",
    "        loss_t = F.binary_cross_entropy(output.float(), target.float(),reduction = 'none').sum(dim = 0)\n",
    "        loss_sum += np.array([i.item() for i in loss_t])\n",
    "        label = output.ge(0.5).float()\n",
    "        acc_t = (target == label).float().sum(dim=0) / (label.shape[0])\n",
    "        acc_sum += np.array([i.item() for i in acc_t])\n",
    "    summary['loss'] = loss_sum / steps\n",
    "    summary['acc'] = acc_sum / steps\n",
    "    predlist = [output_t[:,i].cpu().numpy() for i in range(output_t.shape[1])]\n",
    "    true_list = [target_t[:,i].cpu().numpy() for i in range(target_t.shape[1])]\n",
    "    return summary, predlist, true_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhH7aWIVvyfm",
    "tags": []
   },
   "source": [
    "# DEBUGGING TEST EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "MmMSqAiewAT9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "oK7rYC7kvyfn"
   },
   "outputs": [],
   "source": [
    "# model = EfficientNet_AG(3,5)\n",
    "# learning_rate = 0.001\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer , step_size = 1, gamma = .1)\n",
    "# device = device = torch.device('cuda:0')\n",
    "# model = model.to(device).train()\n",
    "# if cfg.pre_train is not None:\n",
    "#     if os.path.exists(cfg.pre_train):\n",
    "#         ckpt = torch.load(cfg.pre_train, map_location=device)\n",
    "#         model.module.load_state_dict(ckpt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataloader_train = DataLoader(\n",
    "#     CheXDataset(cfg.train_csv, diseases, data_aug_transforms),\n",
    "#     batch_size=cfg.train_batch_size, num_workers=0,\n",
    "#     drop_last=True, shuffle=True)\n",
    "# dataloader_dev = DataLoader(\n",
    "#     CheXDataset(cfg.dev_csv, diseases),\n",
    "#     batch_size=cfg.dev_batch_size, num_workers=0,\n",
    "#     drop_last=False, shuffle=False)\n",
    "# dev_header = dataloader_dev.dataset._label_header\n",
    "\n",
    "# summary_train = {'epoch': 0, 'step': 0}\n",
    "# summary_dev = {'loss': float('inf'), 'acc': 0.0}\n",
    "# summary, predlist, true_list = test_epoch(summary_dev, cfg, model, dataloader_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Cm_JDC9Xvyfn"
   },
   "outputs": [],
   "source": [
    "# auclist = []\n",
    "# for i in range(len(cfg.num_classes)):\n",
    "#     y_pred = predlist[i]\n",
    "#     y_true = true_list[i]\n",
    "#     fpr, tpr, thresholds = metrics.roc_curve(\n",
    "#         y_true, y_pred, pos_label=1)\n",
    "#     auc = metrics.auc(fpr, tpr)\n",
    "#     auclist.append(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "176uWy0rvyfn",
    "tags": []
   },
   "source": [
    "# Normalization and optiizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "SckT1Kf4vyfn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "A-VincZgvyfn"
   },
   "outputs": [],
   "source": [
    "def get_norm(norm_type, num_features, num_groups=32, eps=1e-5):\n",
    "    if norm_type == 'BatchNorm':\n",
    "        return nn.BatchNorm2d(num_features, eps=eps)\n",
    "    elif norm_type == \"GroupNorm\":\n",
    "        return nn.GroupNorm(num_groups, num_features, eps=eps)\n",
    "    elif norm_type == \"InstanceNorm\":\n",
    "        return nn.InstanceNorm2d(num_features, eps=eps,\n",
    "                                 affine=True, track_running_stats=True)\n",
    "    else:\n",
    "        raise Exception('Unknown Norm Function : {}'.format(norm_type))\n",
    "\n",
    "\n",
    "def get_optimizer(params, cfg):\n",
    "    if cfg.optimizer == 'SGD':\n",
    "        return SGD(params, lr=cfg.lr, momentum=cfg.momentum,\n",
    "                   weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == 'Adadelta':\n",
    "        return Adadelta(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == 'Adagrad':\n",
    "        return Adagrad(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == 'Adam':\n",
    "        return Adam(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == 'RMSprop':\n",
    "        return RMSprop(params, lr=cfg.lr, momentum=cfg.momentum,\n",
    "                       weight_decay=cfg.weight_decay)\n",
    "    else:\n",
    "        raise Exception('Unknown optimizer : {}'.format(cfg.optimizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O05Qop2Vvyfn"
   },
   "source": [
    "# Training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hvp3Z1Uwvyfo",
    "outputId": "d02fffcc-93ac-4047-9cf4-1d249d459de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/13963 [00:31<2:24:18,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:17:06, Train, Epoch : 1, Step : 50, Loss : 7.84660 9.16704 8.75501 10.15214 10.87957,Acc : 0.834 0.723 0.786 0.671 0.554, Run Time : 31.08 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/13963 [01:01<2:23:07,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:17:37, Train, Epoch : 1, Step : 100, Loss : 6.47248 8.86148 7.76486 9.95550 10.55679,Acc : 0.864 0.718 0.824 0.696 0.578, Run Time : 30.88 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 150/13963 [01:33<2:31:27,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:18:09, Train, Epoch : 1, Step : 150, Loss : 6.50602 8.69458 7.74134 9.67183 10.49005,Acc : 0.866 0.730 0.818 0.711 0.623, Run Time : 32.04 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 199/13963 [02:04<2:18:27,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:18:40, Train, Epoch : 1, Step : 200, Loss : 6.80054 8.89685 7.79381 9.71305 9.98148,Acc : 0.850 0.715 0.814 0.704 0.677, Run Time : 30.85 sec2021-12-28 03:18:48, Dev, Step : 200, Loss : 0.68582 0.58310 0.33593 0.69204 0.58601, Acc : 0.709 0.808 0.859 0.658 0.739, Auc : 0.725 0.685 0.830 0.742 0.716, Mean auc: 0.740 Run Time : 8.35 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 250/13963 [02:45<2:18:46,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:19:21, Train, Epoch : 1, Step : 250, Loss : 7.07206 8.91593 7.60384 9.46492 10.28708,Acc : 0.843 0.691 0.823 0.714 0.640, Run Time : 40.97 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 300/13963 [03:16<2:20:04,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:19:52, Train, Epoch : 1, Step : 300, Loss : 7.18004 9.16174 7.45591 9.63777 9.76497,Acc : 0.834 0.691 0.819 0.695 0.649, Run Time : 30.85 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 350/13963 [03:47<2:19:13,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:20:23, Train, Epoch : 1, Step : 350, Loss : 7.39857 8.64982 7.31961 9.41769 9.78335,Acc : 0.834 0.731 0.828 0.719 0.675, Run Time : 30.87 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 399/13963 [04:17<2:20:56,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:20:54, Train, Epoch : 1, Step : 400, Loss : 6.21436 8.48931 7.97937 9.40418 9.99700,Acc : 0.871 0.738 0.797 0.709 0.671, Run Time : 30.74 sec2021-12-28 03:21:02, Dev, Step : 400, Loss : 1.07121 0.44520 0.36807 0.62305 0.51800, Acc : 0.709 0.808 0.859 0.658 0.765, Auc : 0.710 0.737 0.812 0.768 0.821, Mean auc: 0.770 Run Time : 8.38 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 450/13963 [04:58<2:19:48,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:21:34, Train, Epoch : 1, Step : 450, Loss : 7.48634 8.77509 8.03914 9.83933 9.87231,Acc : 0.833 0.725 0.794 0.681 0.674, Run Time : 40.21 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 500/13963 [05:29<2:19:01,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:22:05, Train, Epoch : 1, Step : 500, Loss : 7.16968 9.05170 8.00984 9.89536 10.19202,Acc : 0.836 0.704 0.799 0.690 0.651, Run Time : 31.06 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 550/13963 [06:00<2:25:21,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:22:36, Train, Epoch : 1, Step : 550, Loss : 6.53916 8.42208 7.63544 9.16853 10.10618,Acc : 0.861 0.733 0.820 0.728 0.644, Run Time : 31.21 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 599/13963 [06:31<2:22:52,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:23:07, Train, Epoch : 1, Step : 600, Loss : 7.32472 8.63645 7.27325 9.92193 9.81786,Acc : 0.833 0.734 0.828 0.679 0.677, Run Time : 31.17 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 600/13963 [06:40<11:42:55,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:23:16, Dev, Step : 600, Loss : 0.90179 0.50798 0.62047 0.79304 0.64121, Acc : 0.709 0.808 0.859 0.658 0.714, Auc : 0.592 0.663 0.790 0.684 0.722, Mean auc: 0.690 Run Time : 8.38 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 650/13963 [07:11<2:19:11,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:23:47, Train, Epoch : 1, Step : 650, Loss : 7.05876 8.36676 7.62603 9.43320 9.72372,Acc : 0.836 0.731 0.809 0.706 0.666, Run Time : 39.45 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 700/13963 [07:42<2:16:47,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:24:17, Train, Epoch : 1, Step : 700, Loss : 7.03445 9.04149 7.98856 9.73023 9.69215,Acc : 0.836 0.695 0.801 0.684 0.664, Run Time : 30.71 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 750/13963 [08:13<2:15:48,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:24:49, Train, Epoch : 1, Step : 750, Loss : 7.21180 8.51714 7.50776 9.27268 9.63504,Acc : 0.826 0.730 0.823 0.726 0.674, Run Time : 31.56 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 799/13963 [08:44<2:13:33,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:25:20, Train, Epoch : 1, Step : 800, Loss : 6.78381 8.51605 7.31867 9.73777 9.70057,Acc : 0.848 0.723 0.818 0.686 0.685, Run Time : 31.40 sec2021-12-28 03:25:29, Dev, Step : 800, Loss : 0.62709 0.43762 0.33764 0.56114 0.45674, Acc : 0.709 0.816 0.859 0.658 0.774, Auc : 0.731 0.762 0.834 0.791 0.831, Mean auc: 0.790 Run Time : 8.38 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 850/13963 [09:25<2:11:02,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:26:01, Train, Epoch : 1, Step : 850, Loss : 6.78010 8.71950 7.59416 9.79704 9.74620,Acc : 0.849 0.721 0.815 0.682 0.652, Run Time : 40.24 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 900/13963 [09:56<2:13:52,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:26:31, Train, Epoch : 1, Step : 900, Loss : 6.35112 8.02725 7.30982 9.51134 9.51817,Acc : 0.864 0.741 0.820 0.713 0.682, Run Time : 30.89 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 950/13963 [10:26<2:15:07,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:27:02, Train, Epoch : 1, Step : 950, Loss : 6.83730 8.93645 7.35024 9.81524 9.84543,Acc : 0.843 0.695 0.823 0.676 0.656, Run Time : 30.76 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 999/13963 [10:57<2:17:32,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:27:34, Train, Epoch : 1, Step : 1000, Loss : 7.13383 8.63724 7.66173 9.43958 9.69945,Acc : 0.838 0.718 0.810 0.703 0.667, Run Time : 31.32 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1000/13963 [11:06<11:17:11,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:27:42, Dev, Step : 1000, Loss : 0.65614 0.39247 0.34970 0.61855 0.50103, Acc : 0.709 0.808 0.859 0.658 0.735, Auc : 0.581 0.817 0.854 0.812 0.817, Mean auc: 0.776 Run Time : 8.37 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1050/13963 [11:37<2:17:36,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:28:13, Train, Epoch : 1, Step : 1050, Loss : 7.25939 8.10673 7.63391 9.39912 9.38639,Acc : 0.830 0.728 0.815 0.708 0.681, Run Time : 39.58 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1100/13963 [12:08<2:12:41,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:28:44, Train, Epoch : 1, Step : 1100, Loss : 6.81973 8.54125 7.51915 9.04589 9.48685,Acc : 0.848 0.728 0.815 0.721 0.682, Run Time : 30.85 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1150/13963 [12:39<2:17:54,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:29:15, Train, Epoch : 1, Step : 1150, Loss : 6.86506 8.28351 7.04903 9.31590 9.15529,Acc : 0.843 0.723 0.840 0.716 0.698, Run Time : 31.08 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 1199/13963 [13:10<2:17:03,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:29:46, Train, Epoch : 1, Step : 1200, Loss : 7.30246 7.79383 7.28047 9.62012 9.31397,Acc : 0.824 0.759 0.823 0.691 0.691, Run Time : 31.13 sec2021-12-28 03:29:55, Dev, Step : 1200, Loss : 0.67556 0.35776 0.32382 0.55211 0.47038, Acc : 0.709 0.833 0.859 0.658 0.761, Auc : 0.744 0.845 0.864 0.775 0.857, Mean auc: 0.817 Run Time : 8.38 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1250/13963 [13:51<2:15:07,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 03:30:27, Train, Epoch : 1, Step : 1250, Loss : 6.79328 8.14948 7.58918 9.33183 8.86160,Acc : 0.849 0.745 0.801 0.699 0.728, Run Time : 41.08 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1271/13963 [14:05<2:13:24,  1.59it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "from model.classifier import Classifier\n",
    "\n",
    "if not os.path.exists(cfg.save_path):\n",
    "    os.mkdir(cfg.save_path)\n",
    "if cfg.logtofile is True:\n",
    "    logging.basicConfig(filename=cfg.save_path + '/log.txt',\n",
    "                        filemode=\"w\", level=logging.INFO)\n",
    "else:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = EfficientNet_AG(3,5)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "Lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer , step_size = 1, gamma = .1)\n",
    "model = model.to(device).train()\n",
    "if cfg.pre_train is not None:\n",
    "    if os.path.exists(cfg.pre_train):\n",
    "        ckpt = torch.load(cfg.pre_train, map_location=device)\n",
    "        model.module.load_state_dict(ckpt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    CheXDataset(cfg.train_csv, diseases, data_aug_transforms),\n",
    "    batch_size=cfg.train_batch_size, num_workers=0,\n",
    "    drop_last=True, shuffle=True)\n",
    "dataloader_dev = DataLoader(\n",
    "    CheXDataset(cfg.dev_csv, diseases),\n",
    "    batch_size=cfg.dev_batch_size, num_workers=0,\n",
    "    drop_last=False, shuffle=False)\n",
    "dev_header = dataloader_dev.dataset._label_header\n",
    "\n",
    "summary_train = {'epoch': 0, 'step': 0}\n",
    "summary_dev = {'loss': float('inf'), 'acc': 0.0}\n",
    "summary_writer = SummaryWriter(cfg.save_path)\n",
    "epoch_start = 0\n",
    "best_dict = {\n",
    "    \"acc_dev_best\": 0.0,\n",
    "    \"auc_dev_best\": 0.0,\n",
    "    \"loss_dev_best\": float('inf'),\n",
    "    \"fused_dev_best\": 0.0,\n",
    "    \"best_idx\": 1}\n",
    "\n",
    "if cfg.resume:\n",
    "    ckpt_path = os.path.join(cfg.save_path, 'train.tar')\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.module.load_state_dict(ckpt['state_dict'])\n",
    "    summary_train = {'epoch': ckpt['epoch'], 'step': ckpt['step']}\n",
    "    best_dict['acc_dev_best'] = ckpt['acc_dev_best']\n",
    "    best_dict['loss_dev_best'] = ckpt['loss_dev_best']\n",
    "    best_dict['auc_dev_best'] = ckpt['auc_dev_best']\n",
    "    epoch_start = ckpt['epoch']\n",
    "\n",
    "for epoch in range(epoch_start, cfg.epoch):\n",
    "    # lr = lr_schedule(cfg.lr, cfg.lr_factor, summary_train['epoch'],\n",
    "    #                  cfg.lr_epochs)\n",
    "    \n",
    "\n",
    "    summary_train, best_dict = train_epoch(\n",
    "        summary_train, summary_dev, cfg, model,\n",
    "        dataloader_train, dataloader_dev, optimizer,\n",
    "        summary_writer, best_dict, dev_header)\n",
    "    Lr_scheduler.step()\n",
    "    time_now = time.time()\n",
    "    summary_dev, predlist, true_list = test_epoch(\n",
    "        summary_dev, cfg, model, dataloader_dev)\n",
    "    time_spent = time.time() - time_now\n",
    "\n",
    "    auclist = []\n",
    "    for i in range(len(cfg.num_classes)):\n",
    "        y_pred = predlist[i]\n",
    "        y_true = true_list[i]\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(\n",
    "            y_true, y_pred, pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        auclist.append(auc)\n",
    "    summary_dev['auc'] = np.array(auclist)\n",
    "\n",
    "    loss_dev_str = ' '.join(map(lambda x: '{:.5f}'.format(x),\n",
    "                                summary_dev['loss']))\n",
    "    acc_dev_str = ' '.join(map(lambda x: '{:.3f}'.format(x),\n",
    "                               summary_dev['acc']))\n",
    "    auc_dev_str = ' '.join(map(lambda x: '{:.3f}'.format(x),\n",
    "                               summary_dev['auc']))\n",
    "\n",
    "    logging.info(\n",
    "        '{}, Dev, Step : {}, Loss : {}, Acc : {}, Auc : {},'\n",
    "        'Mean auc: {:.3f} ''Run Time : {:.2f} sec' .format(\n",
    "            time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            summary_train['step'],\n",
    "            loss_dev_str,\n",
    "            acc_dev_str,\n",
    "            auc_dev_str,\n",
    "            summary_dev['auc'].mean(),\n",
    "            time_spent))\n",
    "\n",
    "    for t in range(len(cfg.num_classes)):\n",
    "        summary_writer.add_scalar(\n",
    "            'dev/loss_{}'.format(dev_header[t]), summary_dev['loss'][t],\n",
    "            summary_train['step'])\n",
    "        summary_writer.add_scalar(\n",
    "            'dev/acc_{}'.format(dev_header[t]), summary_dev['acc'][t],\n",
    "            summary_train['step'])\n",
    "        summary_writer.add_scalar(\n",
    "            'dev/auc_{}'.format(dev_header[t]), summary_dev['auc'][t],\n",
    "            summary_train['step'])\n",
    "\n",
    "    save_best = False\n",
    "\n",
    "    mean_acc = summary_dev['acc'][cfg.save_index].mean()\n",
    "    if mean_acc >= best_dict['acc_dev_best']:\n",
    "        best_dict['acc_dev_best'] = mean_acc\n",
    "        if cfg.best_target == 'acc':\n",
    "            save_best = True\n",
    "\n",
    "    mean_auc = summary_dev['auc'][cfg.save_index].mean()\n",
    "    if mean_auc >= best_dict['auc_dev_best']:\n",
    "        best_dict['auc_dev_best'] = mean_auc\n",
    "        if cfg.best_target == 'auc':\n",
    "            save_best = True\n",
    "\n",
    "    mean_loss = summary_dev['loss'][cfg.save_index].mean()\n",
    "    if mean_loss <= best_dict['loss_dev_best']:\n",
    "        best_dict['loss_dev_best'] = mean_loss\n",
    "        if cfg.best_target == 'loss':\n",
    "            save_best = True\n",
    "\n",
    "    if save_best:\n",
    "        torch.save(\n",
    "            {'epoch': summary_train['epoch'],\n",
    "             'step': summary_train['step'],\n",
    "             'acc_dev_best': best_dict['acc_dev_best'],\n",
    "             'auc_dev_best': best_dict['auc_dev_best'],\n",
    "             'loss_dev_best': best_dict['loss_dev_best'],\n",
    "             'state_dict': model.state_dict()},\n",
    "            os.path.join(cfg.save_path,\n",
    "                         'best{}.tar'.format(best_dict['best_idx']))\n",
    "        )\n",
    "        best_dict['best_idx'] += 1\n",
    "        if best_dict['best_idx'] > cfg.save_top_k:\n",
    "            best_dict['best_idx'] = 1\n",
    "        logging.info(\n",
    "            '{}, Best, Step : {}, Loss : {}, Acc : {},'\n",
    "            'Auc :{},Best Auc : {:.3f}' .format(\n",
    "                time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                summary_train['step'],\n",
    "                loss_dev_str,\n",
    "                acc_dev_str,\n",
    "                auc_dev_str,\n",
    "                best_dict['auc_dev_best']))\n",
    "    torch.save({'epoch': summary_train['epoch'],\n",
    "                'step': summary_train['step'],\n",
    "                'acc_dev_best': best_dict['acc_dev_best'],\n",
    "                'auc_dev_best': best_dict['auc_dev_best'],\n",
    "                'loss_dev_best': best_dict['loss_dev_best'],\n",
    "                'state_dict': model.state_dict()},\n",
    "               os.path.join(cfg.save_path, 'train.tar'))\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wezQywgJvyfo"
   },
   "outputs": [],
   "source": [
    "cfg.save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAgOxFxuvyfp"
   },
   "outputs": [],
   "source": [
    "cfg.logtofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDML8Zw0vyfp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UNYN-5hvyfp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chexpert_dev_ag.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
