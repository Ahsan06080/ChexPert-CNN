{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "import cv2\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from PIL import Image\n",
    "from data.imgaug import GetTransforms\n",
    "from data.utils import transform\n",
    "np.random.seed(0)\n",
    "from easydict import EasyDict as edict\n",
    "import json\n",
    "cfg_path = 'E:/Chexpert/config/example.json'\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "from shutil import copyfile\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adadelta, Adagrad, Adam, RMSprop\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "data_aug_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=(-180,180)),\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 2)),#new\n",
    "        transforms.RandomAutocontrast(p=0.1),\n",
    "\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "class CheXDataset(Dataset):\n",
    "    def __init__(self, csv_path, diseases,transform=None, make_one=1, mode='train'):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        self.df = self.df.fillna(0)\n",
    "        self.make_one = make_one\n",
    "        header = list(df.columns)\n",
    "        self._label_header = [\n",
    "                header[7],\n",
    "                header[10],\n",
    "                header[11],\n",
    "                header[13],\n",
    "                header[15]]\n",
    "        self.defult_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.df['Path'].values[idx]).convert('RGB') \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else :\n",
    "            image = self.defult_transform(image)\n",
    "            \n",
    "  \n",
    "        labels = np.zeros(len(diseases))\n",
    "        \n",
    "        if int(self.df['No Finding'].values[idx]) == 1  :\n",
    "            labels = labels\n",
    "        else:\n",
    "            for i in range(len(labels)):\n",
    "                if self.df[diseases[i]].values[idx] != -1 :\n",
    "                    labels[i] = self.df[diseases[i]].values[idx]\n",
    "                elif self.make_one :\n",
    "                    labels[i] = 1\n",
    "                else :\n",
    "                    labels[i] = 0  \n",
    "                \n",
    "                \n",
    "        return image,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('E:/Chexpert/config/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('E:/Chexpert/config/train.csv')\n",
    "csv_path = 'E:/Chexpert/config/train.csv'\n",
    "diseases = ['Cardiomegaly', 'Edema', 'Consolidation', 'Atelectasis', 'Pleural_Effusion']\n",
    "dataset = CheXDataset(csv_path, diseases, data_aug_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov 11 23:26:04 2020\n",
    "\n",
    "@author: Ahsan\n",
    "\"\"\"\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from collections import OrderedDict\n",
    "import efficientnet_pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "__all__ = ['EfficientNet', 'EfficientNet_AG']\n",
    "\n",
    "def my_efficientnet_forward(self, inputs):\n",
    "    \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
    "    bs = inputs.size(0)\n",
    "    \n",
    "    # Convolution layers\n",
    "    x = self.extract_features(inputs)\n",
    "    \n",
    "    # Pooling and final linear layer\n",
    "    #print(x.shape)\n",
    "    \n",
    "    return x\n",
    "\n",
    "efficientnet_pytorch.EfficientNet.forward.__code__ = my_efficientnet_forward.__code__\n",
    "\n",
    "def EfficientNet_AG(compound_coeff=0, num_classes= 1000, **kwargs):\n",
    "    r\"\"\"Densenet-121 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = EfficientNet_(num_init_features=64, compound_coeff=compound_coeff, num_classes=num_classes )\n",
    "    return model\n",
    "\n",
    "class EfficientNet_(nn.Module) :\n",
    "    \n",
    "    def __init__(self,num_init_features=3, compound_coeff = 0, weights_path=None, num_classes=5) :\n",
    "        super(EfficientNet_, self).__init__()\n",
    "        \n",
    "        model= EfficientNet.from_pretrained(f'efficientnet-b{compound_coeff}',in_channels=3)\n",
    "        \n",
    "        self.model = model\n",
    "        self.model.add_module('norm5', nn.BatchNorm2d(1536))\n",
    "        self.classifier = nn.Linear(1536, num_classes)\n",
    "    \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "#        features = self.features(x)\n",
    "#        print(features.shape)\n",
    "        features = self.model(x)\n",
    "#        features = self.bn(features) \n",
    "        #print(features.shape)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out_after_pooling = F.avg_pool2d(out, kernel_size=7, stride=7).view(features.size(0), -1)\n",
    "        #print(out_after_pooling.shape)\n",
    "        out = self.classifier(out_after_pooling)\n",
    "        out = self.Sigmoid(out)\n",
    "        #print(out.shape)                   \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('E:/Chexpert/config/train.csv')\n",
    "csv_path = 'E:/Chexpert/config/train.csv'\n",
    "diseases = ['Cardiomegaly', 'Edema', 'Consolidation', 'Atelectasis', 'Pleural_Effusion']\n",
    "dataset = CheXDataset(csv_path, diseases, data_aug_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    CheXDataset(cfg.train_csv, diseases, data_aug_transforms),\n",
    "    batch_size=cfg.train_batch_size, num_workers=0,\n",
    "    drop_last=True, shuffle=True)\n",
    "a,b = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([b,b],dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet_AG(3,5)\n",
    "x = model(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.3638, 5.6767, 5.8255, 5.8206, 5.4457])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss_t = F.binary_cross_entropy(x.float(), b.float(),reduction = 'none').sum(dim = 0)\n",
    "loss_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "cfg = {\n",
    "     \"train_csv\": \"E:/Chexpert/config/train.csv\",\n",
    "    \"dev_csv\": \"E:/Chexpert/config/dev.csv\",\n",
    "    \"backbone\": \"densenet121\",\n",
    "    \"width\": 256,\n",
    "    \"height\": 256,\n",
    "    \"long_side\": 256,\n",
    "    \"fix_ratio\": True,\n",
    "    \"pixel_mean\": 128.0,\n",
    "    \"pixel_std\": 64.0,\n",
    "    \"use_pixel_std\": True,\n",
    "    \"use_equalizeHist\": True,\n",
    "    \"use_transforms_type\": \"Aug\",\n",
    "    \"gaussian_blur\": 3,\n",
    "    \"border_pad\": \"pixel_mean\",\n",
    "    \"num_classes\": [1,1,1,1,1],\n",
    "    \"batch_weight\": True,\n",
    "    \"enhance_index\": [2,6],\n",
    "    \"enhance_times\": 1,\n",
    "    \"pos_weight\": [1,1,1,1,1],\n",
    "    \"train_batch_size\": 8,\n",
    "    \"dev_batch_size\": 1,\n",
    "    \"pretrained\": True,\n",
    "    \"log_every\": 10,\n",
    "    \"test_every\": 100,\n",
    "    \"epoch\": 3,\n",
    "    \"norm_type\": \"BatchNorm\",\n",
    "    \"global_pool\": \"PCAM\",\n",
    "    \"fc_bn\": False,\n",
    "    \"attention_map\": \"None\",\n",
    "    \"lse_gamma\": 0.5,\n",
    "    \"fc_drop\": 0,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"criterion\": \"BCE\",\n",
    "    \"lr\": 0.001,\n",
    "    \"lr_factor\": 0.1,\n",
    "    \"lr_epochs\": [2],\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"best_target\": \"auc\",\n",
    "    \"save_top_k\": 30,\n",
    "    \"save_index\": [0,1,2,3,4],\n",
    "    'save_path' : 'Checkpoints',\n",
    "    'logtofile' : True,\n",
    "    'resume' : False,\n",
    "    'pre_train' : None,\n",
    "    'verbose' :  False\n",
    "}\n",
    "cfg  = edict(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False],\n",
       "        [ True, False, False,  True,  True],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [False,  True, False,  True, False]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.rand((4,5)).ge(0.5)\n",
    "label = torch.ones((4,5))\n",
    "#label = torch.sigmoid(output[index].view(-1)).ge(0.5).float()\n",
    "#acc = (target == label).float().sum() / len(label)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(summary, summary_dev, cfg, model, dataloader,\n",
    "                dataloader_dev, optimizer, summary_writer, best_dict,\n",
    "                dev_header):\n",
    "    torch.set_grad_enabled(True)\n",
    "    model.train()\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    steps = len(dataloader_train)\n",
    "    dataiter = iter(dataloader_train)\n",
    "    num_tasks = 5\n",
    "    label_header = dataloader.dataset._label_header\n",
    "    #num_tasks = len(cfg.num_classes)\n",
    "\n",
    "    time_now = time.time()\n",
    "    loss_sum = np.zeros(num_tasks)\n",
    "    acc_sum = np.zeros(num_tasks)\n",
    "    for step in tqdm.tqdm(range(steps)):\n",
    "        image, target = next(dataiter)\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(image)\n",
    "        # different number of tasks\n",
    "        optimizer.zero_grad()\n",
    "        loss_t = F.binary_cross_entropy(output.float(), target.float(),reduction = 'none').sum(dim = 0)\n",
    "        loss_sum += np.array([i.item() for i in loss_t])\n",
    "        loss =  F.binary_cross_entropy(output.float(), target.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        label = output.ge(0.5).float()\n",
    "        acc_t = (target == label).float().sum(dim=0) / (label.shape[0])\n",
    "        acc_sum += np.array([i.item() for i in acc_t])\n",
    "        summary['step'] += 1\n",
    "\n",
    "        if summary['step'] % cfg.log_every == 0:\n",
    "            time_spent = time.time() - time_now\n",
    "            time_now = time.time()\n",
    "\n",
    "            loss_sum /= cfg.log_every\n",
    "            acc_sum /= cfg.log_every\n",
    "            loss_str = ' '.join(map(lambda x: '{:.5f}'.format(x), loss_sum))\n",
    "            acc_str = ' '.join(map(lambda x: '{:.3f}'.format(x), acc_sum))\n",
    "\n",
    "            logging.info(\n",
    "                '{}, Train, Epoch : {}, Step : {}, Loss : {}, '\n",
    "                'Acc : {}, Run Time : {:.2f} sec'\n",
    "                .format(time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        summary['epoch'] + 1, summary['step'], loss_str,\n",
    "                        acc_str, time_spent))\n",
    "\n",
    "            for t in range(num_tasks):\n",
    "                summary_writer.add_scalar(\n",
    "                    'train/loss_{}'.format(label_header[t]), loss_sum[t],\n",
    "                    summary['step'])\n",
    "                summary_writer.add_scalar(\n",
    "                    'train/acc_{}'.format(label_header[t]), acc_sum[t],\n",
    "                    summary['step'])\n",
    "\n",
    "            loss_sum = np.zeros(num_tasks)\n",
    "            acc_sum = np.zeros(num_tasks)\n",
    "\n",
    "        if summary['step'] % cfg.test_every == 0:\n",
    "            time_now = time.time()\n",
    "            summary_dev, predlist, true_list = test_epoch(\n",
    "                summary_dev, cfg, model, dataloader_dev)\n",
    "            time_spent = time.time() - time_now\n",
    "\n",
    "            auclist = []\n",
    "            for i in range(len(cfg.num_classes)):\n",
    "                y_pred = predlist[i]\n",
    "                y_true = true_list[i]\n",
    "                fpr, tpr, thresholds = metrics.roc_curve(\n",
    "                    y_true, y_pred, pos_label=1)\n",
    "                auc = metrics.auc(fpr, tpr)\n",
    "                auclist.append(auc)\n",
    "            summary_dev['auc'] = np.array(auclist)\n",
    "\n",
    "            loss_dev_str = ' '.join(map(lambda x: '{:.5f}'.format(x),\n",
    "                                        summary_dev['loss']))\n",
    "            acc_dev_str = ' '.join(map(lambda x: '{:.3f}'.format(x),\n",
    "                                       summary_dev['acc']))\n",
    "            auc_dev_str = ' '.join(map(lambda x: '{:.3f}'.format(x),\n",
    "                                       summary_dev['auc']))\n",
    "\n",
    "            logging.info(\n",
    "                '{}, Dev, Step : {}, Loss : {}, Acc : {}, Auc : {},'\n",
    "                'Mean auc: {:.3f} ''Run Time : {:.2f} sec' .format(\n",
    "                    time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    summary['step'],\n",
    "                    loss_dev_str,\n",
    "                    acc_dev_str,\n",
    "                    auc_dev_str,\n",
    "                    summary_dev['auc'].mean(),\n",
    "                    time_spent))\n",
    "\n",
    "            for t in range(len(cfg.num_classes)):\n",
    "                summary_writer.add_scalar(\n",
    "                    'dev/loss_{}'.format(dev_header[t]),\n",
    "                    summary_dev['loss'][t], summary['step'])\n",
    "                summary_writer.add_scalar(\n",
    "                    'dev/acc_{}'.format(dev_header[t]), summary_dev['acc'][t],\n",
    "                    summary['step'])\n",
    "                summary_writer.add_scalar(\n",
    "                    'dev/auc_{}'.format(dev_header[t]), summary_dev['auc'][t],\n",
    "                    summary['step'])\n",
    "\n",
    "            save_best = False\n",
    "            mean_acc = summary_dev['acc'][cfg.save_index].mean()\n",
    "            if mean_acc >= best_dict['acc_dev_best']:\n",
    "                best_dict['acc_dev_best'] = mean_acc\n",
    "                if cfg.best_target == 'acc':\n",
    "                    save_best = True\n",
    "\n",
    "            mean_auc = summary_dev['auc'][cfg.save_index].mean()\n",
    "            if mean_auc >= best_dict['auc_dev_best']:\n",
    "                best_dict['auc_dev_best'] = mean_auc\n",
    "                if cfg.best_target == 'auc':\n",
    "                    save_best = True\n",
    "\n",
    "            mean_loss = summary_dev['loss'][cfg.save_index].mean()\n",
    "            if mean_loss <= best_dict['loss_dev_best']:\n",
    "                best_dict['loss_dev_best'] = mean_loss\n",
    "                if cfg.best_target == 'loss':\n",
    "                    save_best = True\n",
    "\n",
    "            if save_best:\n",
    "                torch.save(\n",
    "                    {'epoch': summary['epoch'],\n",
    "                     'step': summary['step'],\n",
    "                     'acc_dev_best': best_dict['acc_dev_best'],\n",
    "                     'auc_dev_best': best_dict['auc_dev_best'],\n",
    "                     'loss_dev_best': best_dict['loss_dev_best'],\n",
    "                     'state_dict': model.state_dict()},\n",
    "                    os.path.join(cfg.save_path, 'best{}.tar'.format(\n",
    "                        best_dict['best_idx']))\n",
    "                )\n",
    "                best_dict['best_idx'] += 1\n",
    "                if best_dict['best_idx'] > cfg.save_top_k:\n",
    "                    best_dict['best_idx'] = 1\n",
    "                logging.info(\n",
    "                    '{}, Best, Step : {}, Loss : {}, Acc : {},Auc :{},'\n",
    "                    'Best Auc : {:.3f}' .format(\n",
    "                        time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        summary['step'],\n",
    "                        loss_dev_str,\n",
    "                        acc_dev_str,\n",
    "                        auc_dev_str,\n",
    "                        best_dict['auc_dev_best']))\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "    summary['epoch'] += 1\n",
    "\n",
    "    return summary, best_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(summary, cfg, model, dataloader):\n",
    "    torch.set_grad_enabled(False)\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    steps = len(dataloader)\n",
    "    dataiter = iter(dataloader)\n",
    "    num_tasks = len(cfg.num_classes)\n",
    "\n",
    "    loss_sum = np.zeros(num_tasks)\n",
    "    acc_sum = np.zeros(num_tasks)\n",
    "\n",
    "    predlist = list()\n",
    "    true_list = list()\n",
    "    for step in range(steps):\n",
    "        image, target = next(dataiter)\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "         \n",
    "        output = model(image)\n",
    "        if step == 0: \n",
    "            output_t = output\n",
    "            target_t = target\n",
    "        else :\n",
    "            output_t = torch.cat([output_t, output],dim = 0)\n",
    "            target_t = torch.cat([target_t, target],dim = 0)\n",
    "        # different number of tasks\n",
    "        loss_t = F.binary_cross_entropy(x.float(), b.float(),reduction = 'none').sum(dim = 0)\n",
    "        loss_sum += np.array([i.item() for i in loss_t])\n",
    "        label = output.ge(0.5).float()\n",
    "        acc_t = (target == label).float().sum(dim=0) / (label.shape[0])\n",
    "        acc_sum += np.array([i.item() for i in acc_t])\n",
    "    summary['loss'] = loss_sum / steps\n",
    "    summary['acc'] = acc_sum / steps\n",
    "    predlist = [output_t[:,i].cpu().numpy() for i in range(output_t.shape[1])]\n",
    "    true_list = [target_t[:,i].cpu().numpy() for i in range(target_t.shape[1])]\n",
    "    return summary, predlist, true_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DEBUGGING TEST EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet_AG(3,5)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "Lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer , step_size = 1, gamma = .1)\n",
    "model = model.to(device).train()\n",
    "if cfg.pre_train is not None:\n",
    "    if os.path.exists(cfg.pre_train):\n",
    "        ckpt = torch.load(cfg.pre_train, map_location=device)\n",
    "        model.module.load_state_dict(ckpt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    CheXDataset(cfg.train_csv, diseases, data_aug_transforms),\n",
    "    batch_size=cfg.train_batch_size, num_workers=0,\n",
    "    drop_last=True, shuffle=True)\n",
    "dataloader_dev = DataLoader(\n",
    "    CheXDataset(cfg.dev_csv, diseases),\n",
    "    batch_size=cfg.dev_batch_size, num_workers=0,\n",
    "    drop_last=False, shuffle=False)\n",
    "dev_header = dataloader_dev.dataset._label_header\n",
    "\n",
    "summary_train = {'epoch': 0, 'step': 0}\n",
    "summary_dev = {'loss': float('inf'), 'acc': 0.0}\n",
    "summary, predlist, true_list = test_epoch(summary_dev, cfg, model, dataloader_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "auclist = []\n",
    "for i in range(len(cfg.num_classes)):\n",
    "    y_pred = predlist[i]\n",
    "    y_true = true_list[i]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_true, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    auclist.append(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Normalization and optiizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(norm_type, num_features, num_groups=32, eps=1e-5):\n",
    "    if norm_type == 'BatchNorm':\n",
    "        return nn.BatchNorm2d(num_features, eps=eps)\n",
    "    elif norm_type == \"GroupNorm\":\n",
    "        return nn.GroupNorm(num_groups, num_features, eps=eps)\n",
    "    elif norm_type == \"InstanceNorm\":\n",
    "        return nn.InstanceNorm2d(num_features, eps=eps,\n",
    "                                 affine=True, track_running_stats=True)\n",
    "    else:\n",
    "        raise Exception('Unknown Norm Function : {}'.format(norm_type))\n",
    "\n",
    "\n",
    "def get_optimizer(params, cfg):\n",
    "    if cfg.optimizer == 'SGD':\n",
    "        return SGD(params, lr=cfg.lr, momentum=cfg.momentum,\n",
    "                   weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == 'Adadelta':\n",
    "        return Adadelta(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == 'Adagrad':\n",
    "        return Adagrad(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == 'Adam':\n",
    "        return Adam(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == 'RMSprop':\n",
    "        return RMSprop(params, lr=cfg.lr, momentum=cfg.momentum,\n",
    "                       weight_decay=cfg.weight_decay)\n",
    "    else:\n",
    "        raise Exception('Unknown optimizer : {}'.format(cfg.optimizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                            | 84/27926 [01:36<9:54:54,  1.28s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "from model.classifier import Classifier\n",
    "\n",
    "if not os.path.exists(cfg.save_path):\n",
    "    os.mkdir(cfg.save_path)\n",
    "if cfg.logtofile is True:\n",
    "    logging.basicConfig(filename=cfg.save_path + '/log.txt',\n",
    "                        filemode=\"w\", level=logging.INFO)\n",
    "else:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = EfficientNet_AG(3,5)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "Lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer , step_size = 1, gamma = .1)\n",
    "model = model.to(device).train()\n",
    "if cfg.pre_train is not None:\n",
    "    if os.path.exists(cfg.pre_train):\n",
    "        ckpt = torch.load(cfg.pre_train, map_location=device)\n",
    "        model.module.load_state_dict(ckpt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    CheXDataset(cfg.train_csv, diseases, data_aug_transforms),\n",
    "    batch_size=cfg.train_batch_size, num_workers=0,\n",
    "    drop_last=True, shuffle=True)\n",
    "dataloader_dev = DataLoader(\n",
    "    CheXDataset(cfg.dev_csv, diseases),\n",
    "    batch_size=cfg.dev_batch_size, num_workers=0,\n",
    "    drop_last=False, shuffle=False)\n",
    "dev_header = dataloader_dev.dataset._label_header\n",
    "\n",
    "summary_train = {'epoch': 0, 'step': 0}\n",
    "summary_dev = {'loss': float('inf'), 'acc': 0.0}\n",
    "summary_writer = SummaryWriter(cfg.save_path)\n",
    "epoch_start = 0\n",
    "best_dict = {\n",
    "    \"acc_dev_best\": 0.0,\n",
    "    \"auc_dev_best\": 0.0,\n",
    "    \"loss_dev_best\": float('inf'),\n",
    "    \"fused_dev_best\": 0.0,\n",
    "    \"best_idx\": 1}\n",
    "\n",
    "if cfg.resume:\n",
    "    ckpt_path = os.path.join(cfg.save_path, 'train.tar')\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.module.load_state_dict(ckpt['state_dict'])\n",
    "    summary_train = {'epoch': ckpt['epoch'], 'step': ckpt['step']}\n",
    "    best_dict['acc_dev_best'] = ckpt['acc_dev_best']\n",
    "    best_dict['loss_dev_best'] = ckpt['loss_dev_best']\n",
    "    best_dict['auc_dev_best'] = ckpt['auc_dev_best']\n",
    "    epoch_start = ckpt['epoch']\n",
    "\n",
    "for epoch in range(epoch_start, cfg.epoch):\n",
    "    # lr = lr_schedule(cfg.lr, cfg.lr_factor, summary_train['epoch'],\n",
    "    #                  cfg.lr_epochs)\n",
    "    \n",
    "\n",
    "    summary_train, best_dict = train_epoch(\n",
    "        summary_train, summary_dev, cfg, model,\n",
    "        dataloader_train, dataloader_dev, optimizer,\n",
    "        summary_writer, best_dict, dev_header)\n",
    "    Lr_scheduler.step()\n",
    "    time_now = time.time()\n",
    "    summary_dev, predlist, true_list = test_epoch(\n",
    "        summary_dev, cfg, model, dataloader_dev)\n",
    "    time_spent = time.time() - time_now\n",
    "\n",
    "    auclist = []\n",
    "    for i in range(len(cfg.num_classes)):\n",
    "        y_pred = predlist[i]\n",
    "        y_true = true_list[i]\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(\n",
    "            y_true, y_pred, pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        auclist.append(auc)\n",
    "    summary_dev['auc'] = np.array(auclist)\n",
    "\n",
    "    loss_dev_str = ' '.join(map(lambda x: '{:.5f}'.format(x),\n",
    "                                summary_dev['loss']))\n",
    "    acc_dev_str = ' '.join(map(lambda x: '{:.3f}'.format(x),\n",
    "                               summary_dev['acc']))\n",
    "    auc_dev_str = ' '.join(map(lambda x: '{:.3f}'.format(x),\n",
    "                               summary_dev['auc']))\n",
    "\n",
    "    logging.info(\n",
    "        '{}, Dev, Step : {}, Loss : {}, Acc : {}, Auc : {},'\n",
    "        'Mean auc: {:.3f} ''Run Time : {:.2f} sec' .format(\n",
    "            time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            summary_train['step'],\n",
    "            loss_dev_str,\n",
    "            acc_dev_str,\n",
    "            auc_dev_str,\n",
    "            summary_dev['auc'].mean(),\n",
    "            time_spent))\n",
    "\n",
    "    for t in range(len(cfg.num_classes)):\n",
    "        summary_writer.add_scalar(\n",
    "            'dev/loss_{}'.format(dev_header[t]), summary_dev['loss'][t],\n",
    "            summary_train['step'])\n",
    "        summary_writer.add_scalar(\n",
    "            'dev/acc_{}'.format(dev_header[t]), summary_dev['acc'][t],\n",
    "            summary_train['step'])\n",
    "        summary_writer.add_scalar(\n",
    "            'dev/auc_{}'.format(dev_header[t]), summary_dev['auc'][t],\n",
    "            summary_train['step'])\n",
    "\n",
    "    save_best = False\n",
    "\n",
    "    mean_acc = summary_dev['acc'][cfg.save_index].mean()\n",
    "    if mean_acc >= best_dict['acc_dev_best']:\n",
    "        best_dict['acc_dev_best'] = mean_acc\n",
    "        if cfg.best_target == 'acc':\n",
    "            save_best = True\n",
    "\n",
    "    mean_auc = summary_dev['auc'][cfg.save_index].mean()\n",
    "    if mean_auc >= best_dict['auc_dev_best']:\n",
    "        best_dict['auc_dev_best'] = mean_auc\n",
    "        if cfg.best_target == 'auc':\n",
    "            save_best = True\n",
    "\n",
    "    mean_loss = summary_dev['loss'][cfg.save_index].mean()\n",
    "    if mean_loss <= best_dict['loss_dev_best']:\n",
    "        best_dict['loss_dev_best'] = mean_loss\n",
    "        if cfg.best_target == 'loss':\n",
    "            save_best = True\n",
    "\n",
    "    if save_best:\n",
    "        torch.save(\n",
    "            {'epoch': summary_train['epoch'],\n",
    "             'step': summary_train['step'],\n",
    "             'acc_dev_best': best_dict['acc_dev_best'],\n",
    "             'auc_dev_best': best_dict['auc_dev_best'],\n",
    "             'loss_dev_best': best_dict['loss_dev_best'],\n",
    "             'state_dict': model.state_dict()},\n",
    "            os.path.join(cfg.save_path,\n",
    "                         'best{}.tar'.format(best_dict['best_idx']))\n",
    "        )\n",
    "        best_dict['best_idx'] += 1\n",
    "        if best_dict['best_idx'] > cfg.save_top_k:\n",
    "            best_dict['best_idx'] = 1\n",
    "        logging.info(\n",
    "            '{}, Best, Step : {}, Loss : {}, Acc : {},'\n",
    "            'Auc :{},Best Auc : {:.3f}' .format(\n",
    "                time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                summary_train['step'],\n",
    "                loss_dev_str,\n",
    "                acc_dev_str,\n",
    "                auc_dev_str,\n",
    "                best_dict['auc_dev_best']))\n",
    "    torch.save({'epoch': summary_train['epoch'],\n",
    "                'step': summary_train['step'],\n",
    "                'acc_dev_best': best_dict['acc_dev_best'],\n",
    "                'auc_dev_best': best_dict['auc_dev_best'],\n",
    "                'loss_dev_best': best_dict['loss_dev_best'],\n",
    "                'state_dict': model.state_dict()},\n",
    "               os.path.join(cfg.save_path, 'train.tar'))\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.logtofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
