{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Created by: Ahsan\n",
    "###########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n",
    "    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "torch_ver = torch.__version__[:3]\n",
    "\n",
    "__all__ = ['PAM_Module', 'CAM_Module']\n",
    "\n",
    "\n",
    "class PAM_Module(Module):\n",
    "    \"\"\" Position attention module\"\"\"\n",
    "    #Ref from SAGAN\n",
    "    def __init__(self, in_dim):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X (HxW) X (HxW)\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class CAM_Module(Module):\n",
    "    \"\"\" Channel attention module\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "\n",
    "        self.gamma = Parameter(torch.zeros(1))\n",
    "        self.softmax  = Softmax(dim=-1)\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model.py - Model and module class for EfficientNet.\n",
    "   They are built to mirror those in the official TensorFlow implementation.\n",
    "\"\"\"\n",
    "###########################################################################\n",
    "# Created by: Ahsan\n",
    "###########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n",
    "    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "torch_ver = torch.__version__[:3]\n",
    "\n",
    "__all__ = ['PAM_Module', 'CAM_Module']\n",
    "\n",
    "\n",
    "class PAM_Module(Module):\n",
    "    \"\"\" Position attention module\"\"\"\n",
    "    #Ref from SAGAN\n",
    "    def __init__(self, in_dim):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X (HxW) X (HxW)\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class CAM_Module(Module):\n",
    "    \"\"\" Channel attention module\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "\n",
    "        self.gamma = Parameter(torch.zeros(1))\n",
    "        self.softmax  = Softmax(dim=-1)\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "# Author: lukemelas (github username)\n",
    "# Github repo: https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "# With adjustments and added comments by workingcoder (github username).\n",
    "import torch.nn.functional as F\n",
    "from model.backbone.vgg import (vgg19, vgg19_bn)\n",
    "from model.backbone.densenet import (densenet121, densenet169, densenet201)\n",
    "from model.backbone.inception import (inception_v3)\n",
    "from model.global_pool import GlobalPool\n",
    "from model.attention_map import AttentionMap\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from utils_eff import (\n",
    "    round_filters,\n",
    "    round_repeats,\n",
    "    drop_connect,\n",
    "    get_same_padding_conv2d,\n",
    "    get_model_params,\n",
    "    efficientnet_params,\n",
    "    load_pretrained_weights,\n",
    "    Swish,\n",
    "    MemoryEfficientSwish,\n",
    "    calculate_output_image_size\n",
    ")\n",
    "\n",
    "\n",
    "VALID_MODELS = (\n",
    "    'efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3',\n",
    "    'efficientnet-b4', 'efficientnet-b5', 'efficientnet-b6', 'efficientnet-b7',\n",
    "    'efficientnet-b8',\n",
    "\n",
    "    # Support the construction of 'efficientnet-l2' without pretrained weights\n",
    "    'efficientnet-l2'\n",
    ")\n",
    "\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"Mobile Inverted Residual Bottleneck Block.\n",
    "    Args:\n",
    "        block_args (namedtuple): BlockArgs, defined in utils.py.\n",
    "        global_params (namedtuple): GlobalParam, defined in utils.py.\n",
    "        image_size (tuple or list): [image_height, image_width].\n",
    "    References:\n",
    "        [1] https://arxiv.org/abs/1704.04861 (MobileNet v1)\n",
    "        [2] https://arxiv.org/abs/1801.04381 (MobileNet v2)\n",
    "        [3] https://arxiv.org/abs/1905.02244 (MobileNet v3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_args, global_params, image_size=None):\n",
    "        super().__init__()\n",
    "        self._block_args = block_args\n",
    "        self._bn_mom = 1 - global_params.batch_norm_momentum  # pytorch's difference from tensorflow\n",
    "        self._bn_eps = global_params.batch_norm_epsilon\n",
    "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
    "        self.id_skip = block_args.id_skip  # whether to use skip connection and drop connect\n",
    "\n",
    "        # Expansion phase (Inverted Bottleneck)\n",
    "        inp = self._block_args.input_filters  # number of input channels\n",
    "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "            # image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = self._block_args.kernel_size\n",
    "        s = self._block_args.stride\n",
    "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "        self._depthwise_conv = Conv2d(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
    "            kernel_size=k, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "        image_size = calculate_output_image_size(image_size, s)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            Conv2d = get_same_padding_conv2d(image_size=(1, 1))\n",
    "            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n",
    "            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Pointwise convolution phase\n",
    "        final_oup = self._block_args.output_filters\n",
    "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "        self._swish = MemoryEfficientSwish()\n",
    "\n",
    "    def forward(self, inputs, drop_connect_rate=None):\n",
    "        \"\"\"MBConvBlock's forward function.\n",
    "        Args:\n",
    "            inputs (tensor): Input tensor.\n",
    "            drop_connect_rate (bool): Drop connect rate (float, between 0 and 1).\n",
    "        Returns:\n",
    "            Output of this block after processing.\n",
    "        \"\"\"\n",
    "\n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            x = self._expand_conv(inputs)\n",
    "            x = self._bn0(x)\n",
    "            x = self._swish(x)\n",
    "\n",
    "        x = self._depthwise_conv(x)\n",
    "        x = self._bn1(x)\n",
    "        x = self._swish(x)\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_reduce(x_squeezed)\n",
    "            x_squeezed = self._swish(x_squeezed)\n",
    "            x_squeezed = self._se_expand(x_squeezed)\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "\n",
    "        # Pointwise Convolution\n",
    "        x = self._project_conv(x)\n",
    "        x = self._bn2(x)\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
    "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
    "            # The combination of skip connection and drop connect brings about stochastic depth.\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x\n",
    "\n",
    "    def set_swish(self, memory_efficient=True):\n",
    "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export).\n",
    "        Args:\n",
    "            memory_efficient (bool): Whether to use memory-efficient version of swish.\n",
    "        \"\"\"\n",
    "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    \n",
    "    \"\"\"EfficientNet model.\n",
    "       Most easily loaded with the .from_name or .from_pretrained methods.\n",
    "    Args:\n",
    "        blocks_args (list[namedtuple]): A list of BlockArgs to construct blocks.\n",
    "        global_params (namedtuple): A set of GlobalParams shared between blocks.\n",
    "    References:\n",
    "        [1] https://arxiv.org/abs/1905.11946 (EfficientNet)\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> from efficientnet.model import EfficientNet\n",
    "        >>> inputs = torch.rand(1, 3, 224, 224)\n",
    "        >>> model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        >>> model.eval()\n",
    "        >>> outputs = model(inputs)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, feedback =True, upsampler =[0,0,0,0], blocks_args=None, global_params=None):\n",
    "        super().__init__()\n",
    "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
    "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
    "        self.cfg = cfg\n",
    "        self.feedback = feedback\n",
    "        self.global_pool = GlobalPool(cfg)\n",
    "        self.expand = 1\n",
    "        if cfg.global_pool == 'AVG_MAX':\n",
    "            self.expand = 2\n",
    "        elif cfg.global_pool == 'AVG_MAX_LSE':\n",
    "            self.expand = 3\n",
    "        self._global_params = global_params\n",
    "        self._blocks_args = blocks_args \n",
    "        self.upsampler = upsampler\n",
    "        # Batch norm parameters\n",
    "        bn_mom = 1 - self._global_params.batch_norm_momentum\n",
    "        bn_eps = self._global_params.batch_norm_epsilon\n",
    "\n",
    "        # Get stem static or dynamic convolution depending on image size\n",
    "        image_size = global_params.image_size\n",
    "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "\n",
    "        # Stem\n",
    "        in_channels = 3  # rgb\n",
    "        out_channels = round_filters(32, self._global_params)  # number of output channels\n",
    "        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "        image_size = calculate_output_image_size(image_size, 2)\n",
    "        self._init_classifier()\n",
    "        self._init_bn()\n",
    "        self._init_attention_map()\n",
    "        # Build blocks\n",
    "        self._blocks = nn.ModuleList([])\n",
    "        for block_args in self._blocks_args:\n",
    "\n",
    "            # Update block input and output filters based on depth multiplier.\n",
    "            block_args = block_args._replace(\n",
    "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
    "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
    "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
    "            )\n",
    "\n",
    "            # The first block needs to take care of stride and filter size increase.\n",
    "            self._blocks.append(MBConvBlock(block_args, self._global_params, image_size=image_size))\n",
    "            image_size = calculate_output_image_size(image_size, block_args.stride)\n",
    "            if block_args.num_repeat > 1:  # modify block_args to keep same output size\n",
    "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
    "            for _ in range(block_args.num_repeat - 1):\n",
    "                self._blocks.append(MBConvBlock(block_args, self._global_params, image_size=image_size))\n",
    "                # image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1\n",
    "\n",
    "        # Head\n",
    "        in_channels = block_args.output_filters  # output of final block\n",
    "        out_channels = round_filters(1280, self._global_params)\n",
    "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Final linear layer\n",
    "        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        if self._global_params.include_top:\n",
    "            self._dropout = nn.Dropout(self._global_params.dropout_rate)\n",
    "            self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
    "\n",
    "        # set activation to memory efficient swish by default\n",
    "        self._swish = MemoryEfficientSwish()\n",
    "\n",
    "    def set_swish(self, memory_efficient=True):\n",
    "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export).\n",
    "        Args:\n",
    "            memory_efficient (bool): Whether to use memory-efficient version of swish.\n",
    "        \"\"\"\n",
    "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n",
    "        for block in self._blocks:\n",
    "            block.set_swish(memory_efficient)\n",
    "\n",
    "    def extract_endpoints(self, inputs):\n",
    "        \"\"\"Use convolution layer to extract features\n",
    "        from reduction levels i in [1, 2, 3, 4, 5].\n",
    "        Args:\n",
    "            inputs (tensor): Input tensor.\n",
    "        Returns:\n",
    "            Dictionary of last intermediate features\n",
    "            with reduction levels i in [1, 2, 3, 4, 5].\n",
    "            Example:\n",
    "                >>> import torch\n",
    "                >>> from efficientnet.model import EfficientNet\n",
    "                >>> inputs = torch.rand(1, 3, 224, 224)\n",
    "                >>> model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "                >>> endpoints = model.extract_endpoints(inputs)\n",
    "                >>> print(endpoints['reduction_1'].shape)  # torch.Size([1, 16, 112, 112])\n",
    "                >>> print(endpoints['reduction_2'].shape)  # torch.Size([1, 24, 56, 56])\n",
    "                >>> print(endpoints['reduction_3'].shape)  # torch.Size([1, 40, 28, 28])\n",
    "                >>> print(endpoints['reduction_4'].shape)  # torch.Size([1, 112, 14, 14])\n",
    "                >>> print(endpoints['reduction_5'].shape)  # torch.Size([1, 320, 7, 7])\n",
    "                >>> print(endpoints['reduction_6'].shape)  # torch.Size([1, 1280, 7, 7])\n",
    "        \"\"\"\n",
    "        endpoints = dict()\n",
    "\n",
    "        # Stem\n",
    "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
    "        prev_x = x\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):\n",
    "            drop_connect_rate = self._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self._blocks)  # scale drop connect_rate\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "            if prev_x.size(2) > x.size(2):\n",
    "                endpoints['reduction_{}'.format(len(endpoints) + 1)] = prev_x\n",
    "            elif idx == len(self._blocks) - 1:\n",
    "                endpoints['reduction_{}'.format(len(endpoints) + 1)] = x\n",
    "            prev_x = x\n",
    "\n",
    "        # Head\n",
    "        x = self._swish(self._bn1(self._conv_head(x)))\n",
    "        endpoints['reduction_{}'.format(len(endpoints) + 1)] = x\n",
    "\n",
    "        return endpoints\n",
    "\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\"use convolution layer to extract feature .\n",
    "        Args:\n",
    "            inputs (tensor): Input tensor.\n",
    "        Returns:\n",
    "            Output of the final convolution\n",
    "            layer in the efficientnet model.\n",
    "        \"\"\"\n",
    "        # Stem\n",
    "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):\n",
    "            drop_connect_rate = self._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self._blocks)  # scale drop connect_rate\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "            #print(f'{x.shape}+{idx}+{block}')\n",
    "\n",
    "        # Head\n",
    "        x = self._swish(self._bn1(self._conv_head(x)))\n",
    "\n",
    "        return x\n",
    "    def _init_classifier(self):\n",
    "                for index, num_class in enumerate(self.cfg.num_classes):\n",
    "                    setattr(\n",
    "                        self,\n",
    "                        \"fc_\" + str(index),\n",
    "                        nn.Conv2d(\n",
    "                            1536 * self.expand,\n",
    "                            1,\n",
    "                            kernel_size=1,\n",
    "                            stride=1,\n",
    "                            padding=0,\n",
    "                            bias=True))\n",
    "    def _init_attention_map(self):\n",
    "        for index, num_class in enumerate(self.cfg.num_classes):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"attention_map_pam{index}\",\n",
    "                PAM_Module(\n",
    "                    1536))\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"attention_map_cam{index}\",\n",
    "                CAM_Module(\n",
    "                    1536))\n",
    "    def _init_bn(self):\n",
    "        for index, num_class in enumerate(self.cfg.num_classes):\n",
    "            \n",
    "            setattr(\n",
    "                    self,\n",
    "                    \"bn_\" +\n",
    "                    str(index),\n",
    "                    nn.BatchNorm2d(\n",
    "                        1536*\n",
    "                        self.expand))\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"EfficientNet's forward function.\n",
    "           Calls extract_features to extract features, applies final linear layer, and returns logits.\n",
    "        Args:\n",
    "            inputs (tensor): Input tensor.\n",
    "        Returns:\n",
    "            Output of this model after processing.\n",
    "        \"\"\"\n",
    "        # Convolution layers\n",
    "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
    "        if self.feedback :\n",
    "            dim_list = []\n",
    "            prev_x = x\n",
    "            for idx, block in enumerate(self._blocks):\n",
    "                drop_connect_rate = self._global_params.drop_connect_rate\n",
    "                if drop_connect_rate:\n",
    "                    drop_connect_rate *= float(idx) / len(self._blocks)  # scale drop connect_rate\n",
    "                x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "                if prev_x.size(2) > x.size(2):\n",
    "                    print('ssssssssssssssss')\n",
    "                    print(x.shape)\n",
    "                    dim_list.append(x.shape[2])\n",
    "\n",
    "                elif idx == len(self._blocks) - 1:\n",
    "                    print('xxxxxxxxxxxxxxxxx')\n",
    "                    print(x.shape)\n",
    "                prev_x = x \n",
    "        else :\n",
    "            prev_x = x\n",
    "            i = 0\n",
    "            dim_list = []\n",
    "            for idx, block in enumerate(self._blocks):\n",
    "                drop_connect_rate = self._global_params.drop_connect_rate\n",
    "                if drop_connect_rate:\n",
    "                    drop_connect_rate *= float(idx) / len(self._blocks)  # scale drop connect_rate\n",
    "                x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "                if prev_x.size(2) > x.size(2):\n",
    "                    x = x + x*self.upsampler[i]\n",
    "                    i = i+1\n",
    "                    dim_list.append(x.shape[2])\n",
    "                elif idx == len(self._blocks) - 1:\n",
    "                    pass\n",
    "                prev_x = x \n",
    "            #print(f'{x.shape}+{idx}+{block}')\n",
    "\n",
    "        # Head\n",
    "        #print(x.shape)\n",
    "        feat_map  = self._swish(self._bn1(self._conv_head(x)))# 1536x10x10\n",
    "        \n",
    "        #print(feat_map.shape)\n",
    "        # [(N, 1), (N,1),...]\n",
    "        logits = list()\n",
    "        # [(N, H, W), (N, H, W),...]\n",
    "        logit_maps = list()\n",
    "        for index, num_class in enumerate(self.cfg.num_classes):\n",
    "                \n",
    "            attention_map_pam =  getattr(self, 'attention_map_pam' + str(index))\n",
    "            attention_map_cam =  getattr(self, 'attention_map_cam' + str(index))\n",
    "            feat_map_pam = attention_map_pam(feat_map)\n",
    "            feat_map_cam = attention_map_cam(feat_map)\n",
    "            feat_map = feat_map_pam + feat_map_cam\n",
    "            #print(feat_map.shape) --> Nx1536xwxh\n",
    "                \n",
    "            \n",
    "            classifier = getattr(self, \"fc_\" + str(index))\n",
    "            # (N, 1, H, W)\n",
    "            logit_map = None\n",
    "            if not (self.cfg.global_pool == 'AVG_MAX' or\n",
    "                    self.cfg.global_pool == 'AVG_MAX_LSE'):\n",
    "                logit_map = classifier(feat_map)\n",
    "                #print(logit_map.shape)\n",
    "                logit_maps.append(logit_map)\n",
    "            elif self.cfg.global_pool == 'AVG_MAX':\n",
    "                logit_map = classifier(torch,cat([feat_map,feat_map],dim = 1))\n",
    "                logit_maps.append(logit_map)\n",
    "                #print(logit_map.shape)\n",
    "            elif self.cfg.global_pool == 'AVG_MAX_LSE':\n",
    "                logit_map = classifier(torch.cat([feat_map,feat_map,feat_map],dim = 1))\n",
    "                logit_maps.append(logit_map)\n",
    "                #print(logit_map.shape)\n",
    "            # (N, C, 1, 1)\n",
    "            feat = self.global_pool(feat_map, logit_map)\n",
    "                \n",
    "            if self.cfg.fc_bn:\n",
    "                bn = getattr(self, \"bn_\" + str(index))\n",
    "                feat = bn(feat)\n",
    "            feat = F.dropout(feat, p=self.cfg.fc_drop, training=self.training)\n",
    "            # (N, num_class, 1, 1)\n",
    "\n",
    "            logit = classifier(feat)\n",
    "           \n",
    "            # (N, num_class)\n",
    "            logit = logit.squeeze(-1).squeeze(-1)\n",
    "            logits.append(logit)\n",
    "            att = torch.zeros((logit_maps[0].shape[0],logit_maps[0].shape[1],logit_maps[0].shape[2],logit_maps[0].shape[2]))\n",
    "        if self.feedback:\n",
    "            for index in range(len(logit_maps)):\n",
    "                for i in range(logit_maps[0].shape[0]):\n",
    "\n",
    "                    att[i,0,:,:] += torch.sigmoid(logits[index][i])*torch.sigmoid(logit_maps[index][i,0,:,:])\n",
    "            for i in range(len(self.upsampler)) :\n",
    "\n",
    "                Upsampler = nn.Upsample((dim_list[i], dim_list[i]))\n",
    "                a = Upsampler(att)\n",
    "                self.upsampler[i] = a\n",
    "                print('the code reached here')\n",
    "            return (logits, self.upsampler)\n",
    "        else :\n",
    "            return (logits,logit_maps)\n",
    "       \n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, cfg,feedback, upsampler, model_name, in_channels=3, **override_params):\n",
    "        \"\"\"Create an efficientnet model according to name.\n",
    "        Args:\n",
    "            model_name (str): Name for efficientnet.\n",
    "            in_channels (int): Input data's channel number.\n",
    "            override_params (other key word params):\n",
    "                Params to override model's global_params.\n",
    "                Optional key:\n",
    "                    'width_coefficient', 'depth_coefficient',\n",
    "                    'image_size', 'dropout_rate',\n",
    "                    'num_classes', 'batch_norm_momentum',\n",
    "                    'batch_norm_epsilon', 'drop_connect_rate',\n",
    "                    'depth_divisor', 'min_depth'\n",
    "        Returns:\n",
    "            An efficientnet model.\n",
    "        \"\"\"\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        blocks_args, global_params = get_model_params(model_name, override_params)\n",
    "        model = cls(cfg,feedback, upsampler, blocks_args, global_params)\n",
    "        model._change_in_channels(in_channels)\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, cfg,feedback, upsampler, model_name, weights_path=None, advprop=False,\n",
    "                        in_channels=3, num_classes=1000, **override_params):\n",
    "        \"\"\"Create an efficientnet model according to name.\n",
    "        Args:\n",
    "            model_name (str): Name for efficientnet.\n",
    "            weights_path (None or str):\n",
    "                str: path to pretrained weights file on the local disk.\n",
    "                None: use pretrained weights downloaded from the Internet.\n",
    "            advprop (bool):\n",
    "                Whether to load pretrained weights\n",
    "                trained with advprop (valid when weights_path is None).\n",
    "            in_channels (int): Input data's channel number.\n",
    "            num_classes (int):\n",
    "                Number of categories for classification.\n",
    "                It controls the output size for final linear layer.\n",
    "            override_params (other key word params):\n",
    "                Params to override model's global_params.\n",
    "                Optional key:\n",
    "                    'width_coefficient', 'depth_coefficient',\n",
    "                    'image_size', 'dropout_rate',\n",
    "                    'batch_norm_momentum',\n",
    "                    'batch_norm_epsilon', 'drop_connect_rate',\n",
    "                    'depth_divisor', 'min_depth'\n",
    "        Returns:\n",
    "            A pretrained efficientnet model.\n",
    "        \"\"\"\n",
    "        model = cls.from_name(cfg,feedback, upsampler,model_name,  num_classes=num_classes, **override_params)\n",
    "        load_pretrained_weights(model, model_name, weights_path=weights_path,\n",
    "                                load_fc=(num_classes == 1000), advprop=advprop)\n",
    "        model._change_in_channels(in_channels)\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def get_image_size(cls, model_name):\n",
    "        \"\"\"Get the input image size for a given efficientnet model.\n",
    "        Args:\n",
    "            model_name (str): Name for efficientnet.\n",
    "        Returns:\n",
    "            Input image size (resolution).\n",
    "        \"\"\"\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        _, _, res, _ = efficientnet_params(model_name)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _check_model_name_is_valid(cls, model_name):\n",
    "        \"\"\"Validates model name.\n",
    "        Args:\n",
    "            model_name (str): Name for efficientnet.\n",
    "        Returns:\n",
    "            bool: Is a valid name or not.\n",
    "        \"\"\"\n",
    "        if model_name not in VALID_MODELS:\n",
    "            raise ValueError('model_name should be one of: ' + ', '.join(VALID_MODELS))\n",
    "\n",
    "    def _change_in_channels(self, in_channels):\n",
    "        \"\"\"Adjust model's first convolution layer to in_channels, if in_channels not equals 3.\n",
    "        Args:\n",
    "            in_channels (int): Input data's channel number.\n",
    "        \"\"\"\n",
    "        if in_channels != 3:\n",
    "            Conv2d = get_same_padding_conv2d(image_size=self._global_params.image_size)\n",
    "            out_channels = round_filters(32, self._global_params)\n",
    "            self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "cfg = {\n",
    "     \"train_csv\": \"E:/Chexpert/config/train.csv\",\n",
    "    \"dev_csv\": \"E:/Chexpert/config/dev.csv\",\n",
    "    \"backbone\": \"densenet121\",\n",
    "    \"width\": 256,\n",
    "    \"height\": 256,\n",
    "    \"long_side\": 256,\n",
    "    \"fix_ratio\": True,\n",
    "    \"pixel_mean\": 128.0,\n",
    "    \"pixel_std\": 64.0,\n",
    "    \"use_pixel_std\": True,\n",
    "    \"use_equalizeHist\": True,\n",
    "    \"use_transforms_type\": \"Aug\",\n",
    "    \"gaussian_blur\": 3,\n",
    "    \"border_pad\": \"pixel_mean\",\n",
    "    \"num_classes\": [1,1,1,1,1],\n",
    "    \"batch_weight\": True,\n",
    "    \"enhance_index\": [2,6],\n",
    "    \"enhance_times\": 1,\n",
    "    \"pos_weight\": [1,1,1,1,1],\n",
    "    \"train_batch_size\": 8,\n",
    "    \"dev_batch_size\": 1,\n",
    "    \"pretrained\": True,\n",
    "    \"log_every\": 10,\n",
    "    \"test_every\": 100,\n",
    "    \"epoch\": 3,\n",
    "    \"norm_type\": \"BatchNorm\",\n",
    "    \"global_pool\": \"AVG_MAX_LSE\",\n",
    "    \"fc_bn\": False,\n",
    "    \"attention_map\": \"None\",\n",
    "    \"lse_gamma\": 0.5,\n",
    "    \"fc_drop\": 0,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"criterion\": \"BCE\",\n",
    "    \"lr\": 0.001,\n",
    "    \"lr_factor\": 0.1,\n",
    "    \"lr_epochs\": [2],\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"best_target\": \"auc\",\n",
    "    \"save_top_k\": 30,\n",
    "    \"save_index\": [0,1,2,3,4],\n",
    "    'save_path' : 'Checkpoints',\n",
    "    'logtofile' : True,\n",
    "    'resume' : False,\n",
    "    'pre_train' : None,\n",
    "    'verbose' :  False\n",
    "}\n",
    "cfg  = edict(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_fc.weight', '_fc.bias'} Missing keys when loading pretrained weights: ['fc_0.weight', 'fc_0.bias', 'fc_1.weight', 'fc_1.bias', 'fc_2.weight', 'fc_2.bias', 'fc_3.weight', 'fc_3.bias', 'fc_4.weight', 'fc_4.bias', 'bn_0.weight', 'bn_0.bias', 'bn_0.running_mean', 'bn_0.running_var', 'bn_1.weight', 'bn_1.bias', 'bn_1.running_mean', 'bn_1.running_var', 'bn_2.weight', 'bn_2.bias', 'bn_2.running_mean', 'bn_2.running_var', 'bn_3.weight', 'bn_3.bias', 'bn_3.running_mean', 'bn_3.running_var', 'bn_4.weight', 'bn_4.bias', 'bn_4.running_mean', 'bn_4.running_var', 'attention_map_pam0.gamma', 'attention_map_pam0.query_conv.weight', 'attention_map_pam0.query_conv.bias', 'attention_map_pam0.key_conv.weight', 'attention_map_pam0.key_conv.bias', 'attention_map_pam0.value_conv.weight', 'attention_map_pam0.value_conv.bias', 'attention_map_cam0.gamma', 'attention_map_pam1.gamma', 'attention_map_pam1.query_conv.weight', 'attention_map_pam1.query_conv.bias', 'attention_map_pam1.key_conv.weight', 'attention_map_pam1.key_conv.bias', 'attention_map_pam1.value_conv.weight', 'attention_map_pam1.value_conv.bias', 'attention_map_cam1.gamma', 'attention_map_pam2.gamma', 'attention_map_pam2.query_conv.weight', 'attention_map_pam2.query_conv.bias', 'attention_map_pam2.key_conv.weight', 'attention_map_pam2.key_conv.bias', 'attention_map_pam2.value_conv.weight', 'attention_map_pam2.value_conv.bias', 'attention_map_cam2.gamma', 'attention_map_pam3.gamma', 'attention_map_pam3.query_conv.weight', 'attention_map_pam3.query_conv.bias', 'attention_map_pam3.key_conv.weight', 'attention_map_pam3.key_conv.bias', 'attention_map_pam3.value_conv.weight', 'attention_map_pam3.value_conv.bias', 'attention_map_cam3.gamma', 'attention_map_pam4.gamma', 'attention_map_pam4.query_conv.weight', 'attention_map_pam4.query_conv.bias', 'attention_map_pam4.key_conv.weight', 'attention_map_pam4.key_conv.bias', 'attention_map_pam4.value_conv.weight', 'attention_map_pam4.value_conv.bias', 'attention_map_cam4.gamma', '_fc.weight', '_fc.bias']\n",
      "Loaded pretrained weights for efficientnet-b3\n",
      "{'_fc.weight', '_fc.bias'} Missing keys when loading pretrained weights: ['fc_0.weight', 'fc_0.bias', 'fc_1.weight', 'fc_1.bias', 'fc_2.weight', 'fc_2.bias', 'fc_3.weight', 'fc_3.bias', 'fc_4.weight', 'fc_4.bias', 'bn_0.weight', 'bn_0.bias', 'bn_0.running_mean', 'bn_0.running_var', 'bn_1.weight', 'bn_1.bias', 'bn_1.running_mean', 'bn_1.running_var', 'bn_2.weight', 'bn_2.bias', 'bn_2.running_mean', 'bn_2.running_var', 'bn_3.weight', 'bn_3.bias', 'bn_3.running_mean', 'bn_3.running_var', 'bn_4.weight', 'bn_4.bias', 'bn_4.running_mean', 'bn_4.running_var', 'attention_map_pam0.gamma', 'attention_map_pam0.query_conv.weight', 'attention_map_pam0.query_conv.bias', 'attention_map_pam0.key_conv.weight', 'attention_map_pam0.key_conv.bias', 'attention_map_pam0.value_conv.weight', 'attention_map_pam0.value_conv.bias', 'attention_map_cam0.gamma', 'attention_map_pam1.gamma', 'attention_map_pam1.query_conv.weight', 'attention_map_pam1.query_conv.bias', 'attention_map_pam1.key_conv.weight', 'attention_map_pam1.key_conv.bias', 'attention_map_pam1.value_conv.weight', 'attention_map_pam1.value_conv.bias', 'attention_map_cam1.gamma', 'attention_map_pam2.gamma', 'attention_map_pam2.query_conv.weight', 'attention_map_pam2.query_conv.bias', 'attention_map_pam2.key_conv.weight', 'attention_map_pam2.key_conv.bias', 'attention_map_pam2.value_conv.weight', 'attention_map_pam2.value_conv.bias', 'attention_map_cam2.gamma', 'attention_map_pam3.gamma', 'attention_map_pam3.query_conv.weight', 'attention_map_pam3.query_conv.bias', 'attention_map_pam3.key_conv.weight', 'attention_map_pam3.key_conv.bias', 'attention_map_pam3.value_conv.weight', 'attention_map_pam3.value_conv.bias', 'attention_map_cam3.gamma', 'attention_map_pam4.gamma', 'attention_map_pam4.query_conv.weight', 'attention_map_pam4.query_conv.bias', 'attention_map_pam4.key_conv.weight', 'attention_map_pam4.key_conv.bias', 'attention_map_pam4.value_conv.weight', 'attention_map_pam4.value_conv.bias', 'attention_map_cam4.gamma', '_fc.weight', '_fc.bias']\n",
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "model_feedback = EfficientNet.from_pretrained(cfg,True,[0,0,0,0],'efficientnet-b3', num_classes = 5)\n",
    "model = EfficientNet.from_pretrained(cfg,False,None,'efficientnet-b3', num_classes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssssssssssssssss\n",
      "torch.Size([3, 32, 80, 80])\n",
      "ssssssssssssssss\n",
      "torch.Size([3, 48, 40, 40])\n",
      "ssssssssssssssss\n",
      "torch.Size([3, 96, 20, 20])\n",
      "ssssssssssssssss\n",
      "torch.Size([3, 232, 10, 10])\n",
      "xxxxxxxxxxxxxxxxx\n",
      "torch.Size([3, 384, 10, 10])\n",
      "the code reached here\n",
      "the code reached here\n",
      "the code reached here\n",
      "the code reached here\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (56) must match the size of tensor b (80) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e8591a0aec7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_feedback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m320\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m320\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-8223a7b8e574>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    456\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_connect_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mprev_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m                     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsampler\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m                     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m                     \u001b[0mdim_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (56) must match the size of tensor b (80) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "a,b = model_feedback(torch.rand(3,3,320,320))\n",
    "model.upsampler = b\n",
    "c,d = model(torch.rand(3,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-4bd31e5e3b89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'backward'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(100) :\n",
    "    print(i)\n",
    "    assert a==0,'blocks_args should be a list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
