{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from model.backbone.vgg import (vgg19, vgg19_bn)\n",
    "from model.backbone.densenet import (densenet121, densenet169, densenet201)\n",
    "from model.backbone.inception import (inception_v3)\n",
    "from model.global_pool import GlobalPool\n",
    "from model.attention_map import AttentionMap\n",
    "\n",
    "\n",
    "BACKBONES = {'vgg19': vgg19,\n",
    "             'vgg19_bn': vgg19_bn,\n",
    "             'densenet121': densenet121,\n",
    "             'densenet169': densenet169,\n",
    "             'densenet201': densenet201,\n",
    "             'inception_v3': inception_v3}\n",
    "\n",
    "\n",
    "BACKBONES_TYPES = {'vgg19': 'vgg',\n",
    "                   'vgg19_bn': 'vgg',\n",
    "                   'densenet121': 'densenet',\n",
    "                   'densenet169': 'densenet',\n",
    "                   'densenet201': 'densenet',\n",
    "                   'inception_v3': 'inception'}\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1) #320-->160 \n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)#160-->80\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)#80-->40\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)#40-->20\n",
    "        self.conv5 = nn.Conv2d(256, 512, 3, padding=1)#20-->10\n",
    "        self.conv6 = nn.Conv2d(512, 1024, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        \n",
    "        self.upsample3  = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.upsample2  = nn.Upsample(scale_factor=4, mode='nearest')\n",
    "        self.upsample1  = nn.Upsample(scale_factor=8, mode='nearest')\n",
    "        \n",
    "        self.conv4_1x1 = nn.Conv2d(1024, 512, 1)#10\n",
    "        self.conv3_1x1 = nn.Conv2d(1024, 256, 1)#20\n",
    "        self.conv2_1x1 = nn.Conv2d(1024, 128, 1)#40\n",
    "        self.conv1_1x1 = nn.Conv2d(1024, 64, 1)#80\n",
    "        #Decoder\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x_b = F.relu(self.conv6(x))\n",
    "        x_4 = F.relu(self.conv4_1x1(x_b))\n",
    "        x_3 = self.upsample3(x_b)\n",
    "        x_3 = F.relu(self.conv3_1x1(x_3))\n",
    "        x_2 = self.upsample2(x_b)\n",
    "        x_2 = F.relu(self.conv2_1x1(x_2))\n",
    "        x_1 = self.upsample1(x_b)\n",
    "        x_1 = F.relu(self.conv1_1x1(x_1))\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "              \n",
    "        return [x_1,x_2,x_3,x_4], x_b\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.backbone = BACKBONES[cfg.backbone](cfg)\n",
    "        self.global_pool = GlobalPool(cfg)\n",
    "        self.expand = 1\n",
    "        if cfg.global_pool == 'AVG_MAX':\n",
    "            self.expand = 2\n",
    "        elif cfg.global_pool == 'AVG_MAX_LSE':\n",
    "            self.expand = 3\n",
    "        self._init_classifier()\n",
    "        self._init_bn()\n",
    "        self._init_attention_map()\n",
    "        \n",
    "        self.conv0 = self.backbone.features.conv0\n",
    "        self.norm0 = self.backbone.features.norm0\n",
    "        self.relu0 = self.backbone.features.relu0\n",
    "        self.pool0 = self.backbone.features.pool0\n",
    "\n",
    "        ############# Block1-down 64-64  ##############\n",
    "        self.dense_block1 = self.backbone.features.denseblock1\n",
    "        self.trans_block1 = self.backbone.features.transition1\n",
    "\n",
    "        ############# Block2-down 32-32  ##############\n",
    "        self.dense_block2 = self.backbone.features.denseblock2\n",
    "        self.trans_block2 = self.backbone.features.transition2\n",
    "\n",
    "        ############# Block3-down  16-16 ##############\n",
    "        self.dense_block3 = self.backbone.features.denseblock3\n",
    "        self.trans_block3 = self.backbone.features.transition3\n",
    "        \n",
    "        self.dense_block4 = self.backbone.features.denseblock4\n",
    "        \n",
    "        self.conv64 = nn.Conv2d(128,64,1)\n",
    "        self.conv128 = nn.Conv2d(256,128,1)\n",
    "        self.conv256 = nn.Conv2d(512,256,1)\n",
    "        self.conv512 = nn.Conv2d(1024,512,1)\n",
    "        \n",
    "    def _init_classifier(self):\n",
    "        for index, num_class in enumerate(self.cfg.num_classes):\n",
    "            if BACKBONES_TYPES[self.cfg.backbone] == 'vgg':\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"fc_\" + str(index),\n",
    "                    nn.Conv2d(\n",
    "                        512 * self.expand,\n",
    "                        num_class,\n",
    "                        kernel_size=1,\n",
    "                        stride=1,\n",
    "                        padding=0,\n",
    "                        bias=True))\n",
    "            elif BACKBONES_TYPES[self.cfg.backbone] == 'densenet':\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"fc_\" +\n",
    "                    str(index),\n",
    "                    nn.Conv2d(\n",
    "                        self.backbone.num_features *\n",
    "                        self.expand,\n",
    "                        num_class,\n",
    "                        kernel_size=1,\n",
    "                        stride=1,\n",
    "                        padding=0,\n",
    "                        bias=True))\n",
    "            elif BACKBONES_TYPES[self.cfg.backbone] == 'inception':\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"fc_\" + str(index),\n",
    "                    nn.Conv2d(\n",
    "                        2048 * self.expand,\n",
    "                        num_class,\n",
    "                        kernel_size=1,\n",
    "                        stride=1,\n",
    "                        padding=0,\n",
    "                        bias=True))\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    'Unknown backbone type : {}'.format(self.cfg.backbone)\n",
    "                )\n",
    "\n",
    "            classifier = getattr(self, \"fc_\" + str(index))\n",
    "            if isinstance(classifier, nn.Conv2d):\n",
    "                classifier.weight.data.normal_(0, 0.01)\n",
    "                classifier.bias.data.zero_()\n",
    "\n",
    "    def _init_bn(self):\n",
    "        for index, num_class in enumerate(self.cfg.num_classes):\n",
    "            if BACKBONES_TYPES[self.cfg.backbone] == 'vgg':\n",
    "                setattr(self, \"bn_\" + str(index),\n",
    "                        nn.BatchNorm2d(512 * self.expand))\n",
    "            elif BACKBONES_TYPES[self.cfg.backbone] == 'densenet':\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"bn_\" +\n",
    "                    str(index),\n",
    "                    nn.BatchNorm2d(\n",
    "                        self.backbone.num_features *\n",
    "                        self.expand))\n",
    "            elif BACKBONES_TYPES[self.cfg.backbone] == 'inception':\n",
    "                setattr(self, \"bn_\" + str(index),\n",
    "                        nn.BatchNorm2d(2048 * self.expand))\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    'Unknown backbone type : {}'.format(self.cfg.backbone)\n",
    "                )\n",
    "\n",
    "    def _init_attention_map(self):\n",
    "        if BACKBONES_TYPES[self.cfg.backbone] == 'vgg':\n",
    "            setattr(self, \"attention_map\", AttentionMap(self.cfg, 512))\n",
    "        elif BACKBONES_TYPES[self.cfg.backbone] == 'densenet':\n",
    "            for index, num_class in enumerate(self.cfg.num_classes):\n",
    "                setattr(\n",
    "                    self,\n",
    "                    f\"attention_map_{index}\",\n",
    "                    AttentionMap(\n",
    "                        self.cfg,\n",
    "                        self.backbone.num_features))\n",
    "        elif BACKBONES_TYPES[self.cfg.backbone] == 'inception':\n",
    "            setattr(self, \"attention_map\", AttentionMap(self.cfg, 2048))\n",
    "        else:\n",
    "            raise Exception(\n",
    "                'Unknown backbone type : {}'.format(self.cfg.backbone)\n",
    "            )\n",
    "\n",
    "    def cuda(self, device=None):\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (N, C, H, W)\n",
    "        x0 = self.pool0(self.relu0(self.norm0(self.conv0(x))))\n",
    "\n",
    "        ## 64 X 64\n",
    "        x1 = self.dense_block1(x0)\n",
    "        x1 = self.trans_block1(x1)\n",
    "\n",
    "        ###  32x32\n",
    "        x2 = self.trans_block2(self.dense_block2(x1))\n",
    "\n",
    "        ### 16 X 16\n",
    "        x3 = self.trans_block3(self.dense_block3(x2))\n",
    "\n",
    "\n",
    "        ## 8 X 8\n",
    "        feat_map  = self.dense_block4(x3)\n",
    "                \n",
    "              \n",
    "               \n",
    "       \n",
    "            \n",
    "        \n",
    "        #\n",
    "        # feat_map = x\n",
    "        # [(N, 1), (N,1),...]\n",
    "        logits = list()\n",
    "        # [(N, H, W), (N, H, W),...]\n",
    "        logit_maps = 0\n",
    "        for index, num_class in enumerate(self.cfg.num_classes):\n",
    "            if self.cfg.attention_map == \"None\":\n",
    "                \n",
    "                attention_map =  getattr(self, 'attention_map_' + str(index))\n",
    "                feat_map = attention_map(feat_map)\n",
    "                logit_maps +=feat_map\n",
    "            classifier = getattr(self, \"fc_\" + str(index))\n",
    "            # (N, 1, H, W)\n",
    "            \n",
    "            \n",
    "            logit_map = classifier(feat_map)\n",
    "            \n",
    "               \n",
    "            # (N, C, 1, 1)\n",
    "            feat = self.global_pool(feat_map, logit_map)\n",
    "                \n",
    "            if self.cfg.fc_bn:\n",
    "                bn = getattr(self, \"bn_\" + str(index))\n",
    "                feat = bn(feat)\n",
    "            feat = F.dropout(feat, p=self.cfg.fc_drop, training=self.training)\n",
    "            # (N, num_class, 1, 1)\n",
    "\n",
    "            logit = classifier(feat)\n",
    "           \n",
    "            # (N, num_class)\n",
    "            logit = logit.squeeze(-1).squeeze(-1)\n",
    "            logits.append(logit)\n",
    "\n",
    "        return logits, logit_maps\n",
    "    \n",
    "class Classifier2(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super(Classifier2, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.backbone = BACKBONES[cfg.backbone](cfg)\n",
    "        self.global_pool = GlobalPool(cfg)\n",
    "        self.expand = 1\n",
    "        if cfg.global_pool == 'AVG_MAX':\n",
    "            self.expand = 2\n",
    "        elif cfg.global_pool == 'AVG_MAX_LSE':\n",
    "            self.expand = 3\n",
    "        self._init_classifier()\n",
    "        self._init_bn()\n",
    "        self._init_attention_map()\n",
    "        \n",
    "        self.conv0 = self.backbone.features.conv0\n",
    "        self.norm0 = self.backbone.features.norm0\n",
    "        self.relu0 = self.backbone.features.relu0\n",
    "        self.pool0 = self.backbone.features.pool0\n",
    "\n",
    "        ############# Block1-down 64-64  ##############\n",
    "        self.dense_block1 = self.backbone.features.denseblock1\n",
    "        self.trans_block1 = self.backbone.features.transition1\n",
    "\n",
    "        ############# Block2-down 32-32  ##############\n",
    "        self.dense_block2 = self.backbone.features.denseblock2\n",
    "        self.trans_block2 = self.backbone.features.transition2\n",
    "\n",
    "        ############# Block3-down  16-16 ##############\n",
    "        self.dense_block3 = self.backbone.features.denseblock3\n",
    "        self.trans_block3 = self.backbone.features.transition3\n",
    "        \n",
    "        self.dense_block4 = self.backbone.features.denseblock4\n",
    "        \n",
    "        self.upsample3  = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.upsample2  = nn.Upsample(scale_factor=4, mode='nearest')\n",
    "        self.upsample1  = nn.Upsample(scale_factor=8, mode='nearest')\n",
    "        \n",
    "        self.conv64 = nn.Conv2d(1024,64,1)\n",
    "        self.conv128 = nn.Conv2d(1024,128,1)\n",
    "        self.conv256 = nn.Conv2d(1024,256,1)\n",
    "        self.conv512 = nn.Conv2d(1024,512,1)\n",
    "        \n",
    "    def _init_classifier(self):\n",
    "        for index, num_class in enumerate(self.cfg.num_classes):\n",
    "            if BACKBONES_TYPES[self.cfg.backbone] == 'vgg':\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"fc_\" + str(index),\n",
    "                    nn.Conv2d(\n",
    "                        512 * self.expand,\n",
    "                        num_class,\n",
    "                        kernel_size=1,\n",
    "                        stride=1,\n",
    "                        padding=0,\n",
    "                        bias=True))\n",
    "            elif BACKBONES_TYPES[self.cfg.backbone] == 'densenet':\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"fc_\" +\n",
    "                    str(index),\n",
    "                    nn.Conv2d(\n",
    "                        self.backbone.num_features *\n",
    "                        self.expand,\n",
    "                        num_class,\n",
    "                        kernel_size=1,\n",
    "                        stride=1,\n",
    "                        padding=0,\n",
    "                        bias=True))\n",
    "            elif BACKBONES_TYPES[self.cfg.backbone] == 'inception':\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"fc_\" + str(index),\n",
    "                    nn.Conv2d(\n",
    "                        2048 * self.expand,\n",
    "                        num_class,\n",
    "                        kernel_size=1,\n",
    "                        stride=1,\n",
    "                        padding=0,\n",
    "                        bias=True))\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    'Unknown backbone type : {}'.format(self.cfg.backbone)\n",
    "                )\n",
    "\n",
    "            classifier = getattr(self, \"fc_\" + str(index))\n",
    "            if isinstance(classifier, nn.Conv2d):\n",
    "                classifier.weight.data.normal_(0, 0.01)\n",
    "                classifier.bias.data.zero_()\n",
    "\n",
    "    def _init_bn(self):\n",
    "        for index, num_class in enumerate(self.cfg.num_classes):\n",
    "            if BACKBONES_TYPES[self.cfg.backbone] == 'vgg':\n",
    "                setattr(self, \"bn_\" + str(index),\n",
    "                        nn.BatchNorm2d(512 * self.expand))\n",
    "            elif BACKBONES_TYPES[self.cfg.backbone] == 'densenet':\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"bn_\" +\n",
    "                    str(index),\n",
    "                    nn.BatchNorm2d(\n",
    "                        self.backbone.num_features *\n",
    "                        self.expand))\n",
    "            elif BACKBONES_TYPES[self.cfg.backbone] == 'inception':\n",
    "                setattr(self, \"bn_\" + str(index),\n",
    "                        nn.BatchNorm2d(2048 * self.expand))\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    'Unknown backbone type : {}'.format(self.cfg.backbone)\n",
    "                )\n",
    "\n",
    "    def _init_attention_map(self):\n",
    "        if BACKBONES_TYPES[self.cfg.backbone] == 'vgg':\n",
    "            setattr(self, \"attention_map\", AttentionMap(self.cfg, 512))\n",
    "        elif BACKBONES_TYPES[self.cfg.backbone] == 'densenet':\n",
    "            for index, num_class in enumerate(self.cfg.num_classes):\n",
    "                setattr(\n",
    "                    self,\n",
    "                    f\"attention_map_{index}\",\n",
    "                    AttentionMap(\n",
    "                        self.cfg,\n",
    "                        self.backbone.num_features))\n",
    "        elif BACKBONES_TYPES[self.cfg.backbone] == 'inception':\n",
    "            setattr(self, \"attention_map\", AttentionMap(self.cfg, 2048))\n",
    "        else:\n",
    "            raise Exception(\n",
    "                'Unknown backbone type : {}'.format(self.cfg.backbone)\n",
    "            )\n",
    "\n",
    "    def cuda(self, device=None):\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "\n",
    "    def forward(self, x,x_FE):\n",
    "        # (N, C, H, W)\n",
    "        x0 = self.pool0(self.relu0(self.norm0(self.conv0(x))))\n",
    "        x0_FE = self.upsample1(x_FE)\n",
    "        x0_FE = self.conv64(x0_FE)\n",
    "        x0 = x0+x0_FE\n",
    "        ## 64 X 64\n",
    "        \n",
    "        x1 = self.dense_block1(x0)\n",
    "        x1 = self.trans_block1(x1)\n",
    "        x1_FE = self.upsample2(x_FE)\n",
    "        x1_FE = self.conv128(x1_FE)\n",
    "        x1= x1+x1_FE\n",
    "\n",
    "        ###  32x32\n",
    "        x2 = self.trans_block2(self.dense_block2(x1))\n",
    "        x2_FE = self.upsample3(x_FE)\n",
    "        x2_FE = self.conv256(x2_FE)\n",
    "        x2 = x2+x2_FE\n",
    "        ### 16 X 16\n",
    "        x3 = self.trans_block3(self.dense_block3(x2))\n",
    "        x3_FE = self.conv512(x_FE)\n",
    "        x3 = x3+x3_FE\n",
    "\n",
    "\n",
    "        ## 8 X 8\n",
    "        feat_map  = self.dense_block4(x3)\n",
    "                \n",
    "              \n",
    "               \n",
    "       \n",
    "            \n",
    "        \n",
    "        #\n",
    "        # feat_map = x\n",
    "        # [(N, 1), (N,1),...]\n",
    "        logits = list()\n",
    "        # [(N, H, W), (N, H, W),...]\n",
    "        logit_maps = 0\n",
    "        for index, num_class in enumerate(self.cfg.num_classes):\n",
    "            if self.cfg.attention_map == \"None\":\n",
    "                \n",
    "                attention_map =  getattr(self, 'attention_map_' + str(index))\n",
    "                feat_map = attention_map(feat_map)\n",
    "                logit_maps +=feat_map\n",
    "            classifier = getattr(self, \"fc_\" + str(index))\n",
    "            # (N, 1, H, W)\n",
    "            \n",
    "            \n",
    "            logit_map = classifier(feat_map)\n",
    "            \n",
    "               \n",
    "            # (N, C, 1, 1)\n",
    "            feat = self.global_pool(feat_map, logit_map)\n",
    "                \n",
    "            if self.cfg.fc_bn:\n",
    "                bn = getattr(self, \"bn_\" + str(index))\n",
    "                feat = bn(feat)\n",
    "            feat = F.dropout(feat, p=self.cfg.fc_drop, training=self.training)\n",
    "            # (N, num_class, 1, 1)\n",
    "\n",
    "            logit = classifier(feat)\n",
    "           \n",
    "            # (N, num_class)\n",
    "            logit = logit.squeeze(-1).squeeze(-1)\n",
    "            logits.append(logit)\n",
    "\n",
    "        return logits, logit_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "cfg = {\n",
    "     \"train_csv\": \"config/train.csv\",\n",
    "    \"dev_csv\": \"config/dev.csv\",\n",
    "    \"backbone\": \"densenet121\",\n",
    "    \"width\": 320,\n",
    "    \"height\": 320,\n",
    "    \"long_side\": 320,\n",
    "    \"fix_ratio\": True,\n",
    "    \"pixel_mean\": 128.0,\n",
    "    \"pixel_std\": 64.0,\n",
    "    \"use_pixel_std\": True,\n",
    "    \"use_equalizeHist\": True,\n",
    "    \"use_transforms_type\": \"Aug\",\n",
    "    \"gaussian_blur\": 3,\n",
    "    \"border_pad\": \"pixel_mean\",\n",
    "    \"num_classes\": [1,1,1,1,1],\n",
    "    \"batch_weight\": True,\n",
    "    \"enhance_index\": [2,6],\n",
    "    \"enhance_times\": 1,\n",
    "    \"pos_weight\": [1,1,1,1,1],\n",
    "    \"train_batch_size\": 16,\n",
    "    \"dev_batch_size\": 1,\n",
    "    \"pretrained\": True,\n",
    "    \"log_every\": 50,\n",
    "    \"test_every\": 200,\n",
    "    \"epoch\": 30,\n",
    "    \"norm_type\": \"BatchNorm\",\n",
    "    \"global_pool\": \"PCAM\",\n",
    "    \"fc_bn\": False,\n",
    "    \"attention_map\": \"None\",\n",
    "    \"lse_gamma\": 0.5,\n",
    "    \"fc_drop\": 0,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"criterion\": \"BCE\",\n",
    "    \"lr\": 0.001,\n",
    "    \"lr_factor\": 0.1,\n",
    "    \"lr_epochs\": [2],\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"best_target\": \"auc\",\n",
    "    \"save_top_k\": 30,\n",
    "    \"save_index\": [0,1,2,3,4],\n",
    "    'save_path' : '/content/drive/MyDrive/Biomed/efficient net',\n",
    "    'logtofile' : True,\n",
    "    'resume' : False,\n",
    "    'pre_train' : None,\n",
    "    'verbose' :  False\n",
    "}\n",
    "cfg  = edict(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(cfg)\n",
    "model = model.cuda()\n",
    "model_2 = Classifier2(cfg)\n",
    "model_2 = model_2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = model(torch.rand((3,3,320,320)).cuda())\n",
    "x,y = model_2(torch.rand((3,3,320,320)).cuda(),b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024, 10, 10])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 10, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
