{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DEPENDICIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7a993c96cc86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mevaluate_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmy_helper_code\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper_code\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'evaluate_model'"
     ]
    }
   ],
   "source": [
    "##DEPENDICIES\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np, os, sys, joblib\n",
    "import matplotlib.pyplot as pl\n",
    "import pandas as pd\n",
    "import random, os\n",
    "import librosa\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    " #from torch.utils.data import random_split\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import tqdm\n",
    "from evaluate_model import *\n",
    "from my_helper_code import *\n",
    "from helper_code import *\n",
    "from model import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27669.3340)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor((64,320,8))\n",
    "b = torch.tensor((64,320,8))\n",
    "torch.cov(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed: int):\n",
    "    \n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# data accusation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dx_mapping_scored.csv')\n",
    "labels = df['SNOMEDCTCode'].values\n",
    "labels = [str(i) for i in labels]\n",
    "classes = list(labels)\n",
    "train_data_directory = 'training_data'\n",
    "train_header_files, train_recording_files = find_challenge_files(train_data_directory)\n",
    "train_num_recordings = len(train_recording_files)\n",
    "twelve_leads = ('I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6')\n",
    "six_leads = ('I', 'II', 'III', 'aVR', 'aVL', 'aVF')\n",
    "four_leads = ('I', 'II', 'III', 'V2')\n",
    "three_leads = ('I', 'II', 'V2')\n",
    "two_leads = ('I', 'II')\n",
    "lead_sets = (twelve_leads, six_leads, four_leads, three_leads, two_leads)\n",
    "\n",
    "test_data_directory = 'test_data'\n",
    "test_header_files, test_recording_files = find_challenge_files(test_data_directory)\n",
    "test_num_recordings = len(test_recording_files)\n",
    "training_classes = list(labels)\n",
    "test_classes = list(labels)\n",
    "num_classes = len(classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Dataset_separate(Dataset) :\n",
    "    def __init__(self,header_files, recording_files, leads, sample_length, sample_rate = 500) :\n",
    "        super(Dataset,self).__init__()\n",
    "        self.header_files = header_files\n",
    "        self.recording_files = recording_files\n",
    "        self.leads = leads\n",
    "        self.sample_length = sample_length    \n",
    "        self.sample_rate = sample_rate\n",
    "    def __len__(self) :\n",
    "         \n",
    "        return len(self.recording_files)\n",
    "    \n",
    "    def __getitem__(self,index) :\n",
    "        header = load_header(self.header_files[index])\n",
    "        orig_sr = int(header.split(' ')[2])\n",
    "        #print(orig_sr)\n",
    "        recording = load_recording(self.recording_files[index])\n",
    "        recordings = choose_leads(recording, header, self.leads)\n",
    "        data = np.zeros((recordings.shape[0],self.sample_length))\n",
    "        for i in range(len(recordings)):\n",
    "            #print(type(data[i]))\n",
    "            y = librosa.resample(recordings[i].astype(np.float), orig_sr, self.sample_rate, res_type='kaiser_best') \n",
    "            #print(y.shape[0])\n",
    "            if y.shape[0] < self.sample_length :\n",
    "                \n",
    "                data[i,0:y.shape[0]] = y\n",
    "            elif y.shape[0] >= self.sample_length:\n",
    "                data[i] = y[0:self.sample_length]\n",
    "        current_labels = get_labels(header)\n",
    "        #print(current_labels)\n",
    "        labels = np.zeros(( num_classes))\n",
    "        for label in current_labels:\n",
    "            if label in classes:\n",
    "                j = classes.index(label)\n",
    "                labels[j] = 1\n",
    "#         data =recordings[:,0:self.sample_length]\n",
    "       #data = data*10/np.linalg.norm(data)\n",
    "#         for i in range(len(data)):\n",
    "#             data[i] = data[i]/max(abs(data[i]))\n",
    "#         orig_sr = int(header.split(' ')[2])\n",
    "#         for i in range(len(data)):\n",
    "#             #print(type(data[i]))\n",
    "#             data[i] = librosa.resample(data[i].astype(np.float), orig_sr, self.sample_rate, res_type='kaiser_best')        \n",
    "        recording_id = get_recording_id(header)\n",
    "        if data.shape[1] < self.sample_length :\n",
    "            print(data)\n",
    "        return (data,labels,self.header_files[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_length = 4096\n",
    "# data = My_Dataset_separate(train_header_files, train_recording_files, twelve_leads,sample_length)\n",
    "# a,b,c =data[50]\n",
    "# print('-----------------------data----------------')\n",
    "# print(a)\n",
    "# print('----------------------data shape -----------')\n",
    "# print(a.shape)\n",
    "# print('---------------------- labels -----------')\n",
    "# print(b)\n",
    "# print('----------------------labels shape -----------')\n",
    "# print(b.shape)\n",
    "# print('----------------------index -----------')\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_length = 4096\n",
    "BATCH_SIZE  = 128\n",
    "train_dataset = My_Dataset_separate(train_header_files, train_recording_files, twelve_leads,sample_length)\n",
    "test_dataset = My_Dataset_separate(test_header_files, test_recording_files, twelve_leads,sample_length)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1,\n",
    "                             shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model v.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Inception_module(nn.Module) :\n",
    "#     def __init__(self,n_filters_in, n_filters_out, activision_function = 'relu'):\n",
    "#         super(Inception_module,self).__init__()\n",
    "        \n",
    "#         self.n_filters_in = n_filters_in\n",
    "#         self.num_features = n_filters_out\n",
    "#         self.n_filters_out = int(n_filters_out/4)\n",
    "#         self.activision_function = activision_function\n",
    "#         self.conv_1d_1 = nn.Conv1d(self.n_filters_in, self.n_filters_out, kernel_size=1)\n",
    "#         self.conv_1d_7 = nn.Conv1d(self.n_filters_in, self.n_filters_out, kernel_size=7, padding = 3)\n",
    "#         self.conv_1d_17 = nn.Conv1d(self.n_filters_in, self.n_filters_out, kernel_size=17, padding = 8)\n",
    "#         self.conv_1d_1_2 = nn.Conv1d(self.n_filters_in, self.n_filters_out, kernel_size=1, padding = 0)\n",
    "#         self.conv_1d_1_3 = nn.Conv1d(self.n_filters_in, self.n_filters_out, kernel_size=1, padding = 0)\n",
    "#         self.conv_1d_1_4 = nn.Conv1d(self.n_filters_in, self.n_filters_out, kernel_size=1, padding = 0)\n",
    "#         self.maxpool_1d = nn.MaxPool1d(7, stride=1)\n",
    "#         self.batch_norm_1d = nn.BatchNorm1d(num_features = self.num_features )\n",
    "            \n",
    "            \n",
    "#     def forward(self, x) :\n",
    "        \n",
    "#         x_1 = self.conv_1d_1(x)\n",
    "#         #print(x_1.shape)\n",
    "#         x_2 = self.conv_1d_1_2(x)\n",
    "#         x_2 = self.conv_1d_7(x)\n",
    "#         #print(x_2.shape)\n",
    "#         x_3 = self.conv_1d_1_3(x)\n",
    "#         x_3 = self.conv_1d_17(x)\n",
    "#         #print(x_3.shape)\n",
    "#         x_4 = self.maxpool_1d(x)\n",
    "#         x_4 = self.conv_1d_1_4(x)\n",
    "#         #print(x_4.shape)\n",
    "#         x = torch.cat([x_1,x_2,x_3,x_4], dim = 1)\n",
    "#         x = self.batch_norm_1d(x)\n",
    "        \n",
    "#         x = eval(f'F.{self.activision_function}(x)' )\n",
    "        \n",
    "#         return x\n",
    "\n",
    "\n",
    "# class SkipConnection(nn.Module) :\n",
    "#     def __init__(self, down_sample, n_filters_in,n_filters_out):\n",
    "#         super(SkipConnection, self).__init__()\n",
    "        \n",
    "#         self.down_sample = down_sample\n",
    "#         self.n_filters_in = n_filters_in\n",
    "#         self.n_filters_out = n_filters_out\n",
    "#         self.maxpool_1d = nn.MaxPool1d(self.down_sample, stride=self.down_sample)\n",
    "#         self.conv_1d = nn.Conv1d(self.n_filters_in, self.n_filters_out, kernel_size=1, stride=1)\n",
    "#     def forward(self, y) :\n",
    "#         if self.down_sample > 1:\n",
    "#             y = self.maxpool_1d(y)\n",
    "#         elif self.down_sample == 1:\n",
    "#             y = y\n",
    "#         else:\n",
    "#             raise ValueError(\"Number of samples should always decrease.\")\n",
    "#         # Deal with n_filters dimension increase\n",
    "#         if self.n_filters_in != self.n_filters_out:\n",
    "#             # This is one of the two alternatives presented in ResNet paper\n",
    "#             # Other option is to just fill the matrix with zeros.\n",
    "#             y = self.conv_1d(y)\n",
    "#         return y\n",
    "    \n",
    "    \n",
    "    \n",
    "# class BatchNorm_PlusActivision(nn.Module) :\n",
    "#     def __init__(self, n_filters_in, activision_function = 'relu',postactivation_bn= 'True'):\n",
    "#         super(BatchNorm_PlusActivision, self).__init__()\n",
    "        \n",
    "#         self.activision_function = activision_function\n",
    "#         self.postactivation_bn =   postactivation_bn  \n",
    "#         self.n_filters_in = n_filters_in\n",
    "    \n",
    "#         self.batch_norm_1d =nn.BatchNorm1d(num_features = self.n_filters_in)\n",
    "#     def forward(self, x) :\n",
    "#         if self.postactivation_bn : \n",
    "#             x = eval(f'F.{self.activision_function}(x)' )\n",
    "#             x = self.batch_norm_1d(x)\n",
    "#         else :\n",
    "#             x = self.batch_norm_1d(x)\n",
    "#             x = eval(f'F.{self.activision_function}(x)' )\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "# class ResidualUnit(nn.Module):\n",
    "#     def __init__(self,down_sample, n_filters_in, n_filters_out, \n",
    "#                  dropout_keep_prob=0.8, kernel_size=17, preactivation=True,\n",
    "#                  postactivation_bn=False, activation_function='relu', last_layer = 'sigmoid'):\n",
    "#         super(ResidualUnit, self).__init__()\n",
    "                \n",
    "#         self.down_sample = down_sample    \n",
    "#         self.n_filters_in = n_filters_in\n",
    "#         self.n_filters_out = n_filters_out\n",
    "#         self.dropout_rate = 1 - dropout_keep_prob\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.preactivation = preactivation\n",
    "#         self.postactivation_bn = postactivation_bn\n",
    "#         self.activation_function = activation_function\n",
    "#         self.last_layer = last_layer\n",
    "#         self.kernel_size = kernel_size\n",
    "      \n",
    "        \n",
    "        \n",
    "#         self.conv_1d_1 = nn.Conv1d(self.n_filters_in, self.n_filters_out, kernel_size=self.kernel_size, padding = 8)\n",
    "#         self.conv_1d_2 = nn.Conv1d(self.n_filters_out, self.n_filters_out, kernel_size=self.kernel_size, stride = self.down_sample,padding = 8)\n",
    "#         self.skip_connection = SkipConnection(self.down_sample, self.n_filters_in,self.n_filters_out)\n",
    "#         self.batch_norm_plus_activision = BatchNorm_PlusActivision(self.n_filters_out, self.activation_function, self.postactivation_bn)\n",
    "#         self.dropout =  nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "#     def forward(self, x, y):\n",
    "#         y = self.skip_connection(y)\n",
    "#         x = self.conv_1d_1(x)\n",
    "#         x = self.conv_1d_2(x)\n",
    "#         if self.dropout_rate > 0 :\n",
    "#             x = self.dropout(x)\n",
    "#         if self.preactivation:\n",
    "#             x = torch.add(x,y)\n",
    "#             y = x\n",
    "#             x = self.batch_norm_plus_activision(x)\n",
    "#             if self.dropout_rate > 0 :\n",
    "#                  x = self.dropout(x)\n",
    "#         else:\n",
    "#             x = self.batch_norm_plus_activision(x)\n",
    "#             x = torch.add(x,y)\n",
    "#             x = eval(f'F.{self.activision_function}(x)' )\n",
    "#             if self.dropout_rate > 0 :\n",
    "#                     x = self.dropout(x)\n",
    "#             y = x\n",
    "#         return x,y\n",
    "    \n",
    "    \n",
    "# class ResidualNetwork(nn.Module) :\n",
    "#     def __init__(self, n_filters_in,\n",
    "#                  dropout_keep_prob=0.8, kernel_size=17, preactivation=True,\n",
    "#                  postactivation_bn=False, activation_function='relu', last_layer = 'sigmoid'):\n",
    "#         super(ResidualNetwork, self).__init__()\n",
    "          \n",
    "#         self.n_filters_in = n_filters_in\n",
    "#         self.dropout_keep_prob = dropout_keep_prob\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.preactivation = preactivation\n",
    "#         self.postactivation_bn = postactivation_bn\n",
    "#         self.activation_function = activation_function\n",
    "#         self.last_layer = last_layer\n",
    "#         self.kernel_size = kernel_size\n",
    "      \n",
    "        \n",
    "#         #self.conv_1d = nn.Conv1d(n_filters_in,64, kernel_size=self.kernel_size, padding = 8)\n",
    "#         #self.batch_norm_1d = nn.BatchNorm1d(num_features = 64)\n",
    "#         self.inception_unit  =  Inception_module(n_filters_in = self.n_filters_in, n_filters_out = 64, activision_function = 'relu')\n",
    "#         self.residual_unit_1 = ResidualUnit(4,64,128)\n",
    "#         self.residual_unit_2 = ResidualUnit(4,128,196)\n",
    "#         self.residual_unit_3 = ResidualUnit(4,196,256)\n",
    "#         self.residual_unit_4 = ResidualUnit(4,256,320)\n",
    "#         self.linear = nn.Linear(5120, 30)\n",
    "#         self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x) :\n",
    "        \n",
    "#         x = self.inception_unit(x)\n",
    "#         x = eval(f'F.{self.activation_function}(x)' )\n",
    "#         x,y = self.residual_unit_1(x,x)\n",
    "#         x,y = self.residual_unit_2(x,y)\n",
    "#         x,y = self.residual_unit_3(x,y)\n",
    "#         x,_ = self.residual_unit_4(x,y)\n",
    "#         x = x.view(x.size(0),-1)\n",
    "#         x = self.linear(x)\n",
    "#         x = self.Sigmoid(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global pool Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PcamPool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PcamPool, self).__init__()\n",
    "\n",
    "    def forward(self, feat_map, logit_map):\n",
    "        assert logit_map is not None\n",
    "\n",
    "        prob_map = torch.sigmoid(logit_map)\n",
    "        weight_map = prob_map / prob_map.sum(dim=2, keepdim=True)\n",
    "        feat = (feat_map * weight_map).sum(dim=2, keepdim=True)\n",
    "\n",
    "        return feat\n",
    "    \n",
    "class LogSumExpPool(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        super(LogSumExpPool, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, feat_map):\n",
    "        \"\"\"\n",
    "        Numerically stable implementation of the operation\n",
    "        Arguments:\n",
    "            feat_map(Tensor): tensor with shape (N, C, H, W)\n",
    "            return(Tensor): tensor with shape (N, C, 1, 1)\n",
    "        \"\"\"\n",
    "        (N, C, H,) = feat_map.shape\n",
    "\n",
    "        # (N, C, 1, 1) m\n",
    "        m, _ = torch.max(\n",
    "            feat_map, dim=-1, keepdim=True)[0]\n",
    "\n",
    "        # (N, C, H, W) value0\n",
    "        value0 = feat_map - m\n",
    "        area = 1.0 / (H)\n",
    "        g = self.gamma\n",
    "\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        return m + 1 / g * torch.log(area * torch.sum(\n",
    "            torch.exp(g * value0), dim=(-1), keepdim=True))\n",
    "\n",
    "class ExpPool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ExpPool, self).__init__()\n",
    "\n",
    "    def forward(self, feat_map):\n",
    "        \"\"\"\n",
    "        Numerically stable implementation of the operation\n",
    "        Arguments:\n",
    "            feat_map(Tensor): tensor with shape (N, C, H)\n",
    "            return(Tensor): tensor with shape (N, C, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        EPSILON = 1e-7\n",
    "        (N, C, H) = feat_map.shape\n",
    "        m, _ = torch.max(\n",
    "            feat_map, dim=-1, keepdim=True)[0]\n",
    "\n",
    "        # caculate the sum of exp(xi)\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        sum_exp = torch.sum(torch.exp(feat_map - m),\n",
    "                            dim=(-1), keepdim=True)\n",
    "\n",
    "        # prevent from dividing by zero\n",
    "        sum_exp += EPSILON\n",
    "\n",
    "        # caculate softmax in shape of (H,W)\n",
    "        exp_weight = torch.exp(feat_map - m) / sum_exp\n",
    "        weighted_value = feat_map * exp_weight\n",
    "\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        return torch.sum(weighted_value, dim=(-1), keepdim=True)\n",
    "\n",
    "class LinearPool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearPool, self).__init__()\n",
    "\n",
    "    def forward(self, feat_map):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            feat_map(Tensor): tensor with shape (N, C, H)\n",
    "            return(Tensor): tensor with shape (N, C, 1)\n",
    "        \"\"\"\n",
    "        EPSILON = 1e-7\n",
    "        (N, C, H) = feat_map.shape\n",
    "\n",
    "        # sum feat_map's last two dimention into a scalar\n",
    "        # so the shape of sum_input is (N,C,1,1)\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        sum_input = torch.sum(feat_map, dim=(-1), keepdim=True)\n",
    "\n",
    "        # prevent from dividing by zero\n",
    "        sum_input += EPSILON\n",
    "\n",
    "        # caculate softmax in shape of (H,W)\n",
    "        linear_weight = feat_map / sum_input\n",
    "        weighted_value = feat_map * linear_weight\n",
    "\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        return torch.sum(weighted_value, dim=(-1), keepdim=True)   \n",
    "class GlobalPool(nn.Module):\n",
    "\n",
    "    def __init__(self, pool='PCAM'):\n",
    "        super(GlobalPool, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.exp_pool = ExpPool()\n",
    "        self.pcampool = PcamPool()\n",
    "        self.linear_pool = LinearPool()\n",
    "        self.lse_pool = LogSumExpPool(gamma = .9)\n",
    "        self.pool = pool\n",
    "    def cuda(self, device=None):\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "\n",
    "    def forward(self, feat_map, logit_map):\n",
    "        if self.pool == 'AVG':\n",
    "            return self.avgpool(feat_map)\n",
    "        elif self.pool == 'MAX':\n",
    "            return self.maxpool(feat_map)\n",
    "        elif self.pool == 'PCAM':\n",
    "            return self.pcampool(feat_map, logit_map)\n",
    "        elif self.pool == 'AVG_MAX':\n",
    "            a = self.avgpool(feat_map)\n",
    "            b = self.maxpool(feat_map)\n",
    "            return torch.cat((a, b), 1)\n",
    "        elif self.pool == 'AVG_MAX_LSE':\n",
    "            a = self.avgpool(feat_map)\n",
    "            b = self.maxpool(feat_map)\n",
    "            c = self.lse_pool(feat_map)\n",
    "            return torch.cat((a, b, c), 1)\n",
    "        elif self.pool == 'EXP':\n",
    "            return self.exp_pool(feat_map)\n",
    "        elif self.pool == 'LINEAR':\n",
    "            return self.linear_pool(feat_map)\n",
    "        elif self.pool == 'LSE':\n",
    "            return self.lse_pool(feat_map)\n",
    "        else:\n",
    "            raise Exception('Unknown pooling type : {}'\n",
    "                            .format(self.cfg.global_pool))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module) :\n",
    "  def __init__(self, n_class) :\n",
    "    super(Classifier,self).__init__()\n",
    "\n",
    "    self.n_class = n_class\n",
    "    for i in range(self.n_class) :\n",
    "      setattr(self, 'fc_' + str(i+1), nn.Conv1d(320, 512, kernel_size=17,stride = 17, padding = 1))\n",
    "    self.linear = nn.Linear(512,1)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "  def forward(self,x) :\n",
    "    logits = list()\n",
    "\n",
    "    for i in range(self.n_class) :\n",
    "      classifier = getattr(self, 'fc_' + str(i+1))\n",
    "      logit = classifier(x)\n",
    "      \n",
    "      logit = torch.squeeze(logit)\n",
    "      logit = self.linear(logit)\n",
    "      logit = self.sigmoid(logit)\n",
    "      logits.append(logit)\n",
    "    if logits[0].shape[0] > 1 :\n",
    "      output = torch.cat(logits,dim = 1)\n",
    "    else :\n",
    "       output = torch.cat(logits,dim = 0)\n",
    "       \n",
    "    return output\n",
    "\n",
    "\n",
    "class AG_classifier(nn.Module) :\n",
    "  def __init__(self, n_class) :\n",
    "    super(AG_classifier,self).__init__()\n",
    "\n",
    "    self.n_class = n_class\n",
    "    for i in range(self.n_class):\n",
    "        setattr(self,'conv_'+str(i+1), nn.Conv1d(320,128, kernel_size=17,stride = 17, padding = 1))\n",
    "        setattr(self,'fc_'+str(i+1), nn.Linear(128,1))\n",
    "    self.input_layer = 128*self.n_class\n",
    "    self.linear = nn.Linear(self.input_layer,n_class)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "  def forward(self,x):\n",
    "    logits = list()\n",
    "    #attentions = list()\n",
    "    for i in range(self.n_class) :\n",
    "        \n",
    "        classifier = getattr(self, 'conv_' + str(i+1))\n",
    "        logit = classifier(x)\n",
    "        \n",
    "        logit = torch.squeeze(logit)\n",
    "      \n",
    "        attention_ = getattr(self, 'fc_'+str(i+1))\n",
    "        attention_ = self.sigmoid(logit)\n",
    "        logit = torch.mul(logit,attention_)\n",
    "        logits.append(logit)\n",
    "    \n",
    "    output = torch.cat(logits,dim = 1)                        \n",
    "    output = self.linear(output)                        \n",
    "    return output\n",
    "    \n",
    "class Classifier_Pool(nn.Module) :\n",
    "    def __init__(self, n_class) :\n",
    "        super(Classifier_Pool,self).__init__()\n",
    "        self.n_class = n_class\n",
    "        for i in range(self.n_class):\n",
    "            setattr(self,'conv_'+str(i+1), nn.Conv1d(320,1, kernel_size=1,stride = 1, padding = 0))\n",
    "            \n",
    "        self.input_layer = 128*self.n_class\n",
    "        self.linear = nn.Linear(self.input_layer,n_class)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.global_pool = GlobalPool(pool = 'PCAM')\n",
    "        \n",
    "    def forward(self,feat_map):\n",
    "        logits = list()\n",
    "        logit_maps = list()\n",
    "        #attentions = list()\n",
    "        for i in range(self.n_class) :\n",
    "        \n",
    "            classifier = getattr(self, 'conv_' + str(i+1))\n",
    "            logit_map = None\n",
    "            logit_map = classifier(feat_map)\n",
    "            #print(logit_map.shape)\n",
    "            logit_maps.append(logit_map.squeeze())\n",
    "            feat = self.global_pool(feat_map, logit_map)\n",
    "            logit = classifier(feat)\n",
    "            logit = logit.squeeze(-1)\n",
    "            #feat = F.dropout(feat, p=self.fc_drop, training=self.training)\n",
    "            logit = self.sigmoid(logit) \n",
    "            logits.append(logit)\n",
    "            \n",
    "        if logits[0].shape[0] == 1 :\n",
    "            output = torch.cat(logits,dim = 0).t() \n",
    "        else :\n",
    "            output = torch.cat(logits,dim = 1)                        \n",
    "        return output,logit_maps\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "class SkipConnection(nn.Module) :\n",
    "    def __init__(self, down_sample, n_filters_in,n_filters_out):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        \n",
    "        self.down_sample = down_sample\n",
    "        self.n_filters_in = n_filters_in\n",
    "        self.n_filters_out = n_filters_out\n",
    "        self.maxpool_1d = nn.MaxPool1d(self.down_sample, stride=self.down_sample)\n",
    "        self.conv_1d = nn.Conv1d(self.n_filters_in, self.n_filters_out, kernel_size=1, stride=1)\n",
    "    def forward(self, y) :\n",
    "        if self.down_sample > 1:\n",
    "            y = self.maxpool_1d(y)\n",
    "        elif self.down_sample == 1:\n",
    "            y = y\n",
    "        else:\n",
    "            raise ValueError(\"Number of samples should always decrease.\")\n",
    "        # Deal with n_filters dimension increase\n",
    "        if self.n_filters_in != self.n_filters_out:\n",
    "            # This is one of the two alternatives presented in ResNet paper\n",
    "            # Other option is to just fill the matrix with zeros.\n",
    "            y = self.conv_1d(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "    \n",
    "class BatchNorm_PlusActivision(nn.Module) :\n",
    "    def __init__(self, n_filters_in, activision_function = 'relu',postactivation_bn = 'True'):\n",
    "        super(BatchNorm_PlusActivision, self).__init__()\n",
    "        \n",
    "        self.activision_function = activision_function\n",
    "        self.postactivation_bn =   postactivation_bn  \n",
    "        self.n_filters_in = n_filters_in\n",
    "    \n",
    "        self.batch_norm_1d =nn.BatchNorm1d(num_features = self.n_filters_in)\n",
    "    def forward(self, x) :\n",
    "        if self.postactivation_bn : \n",
    "            x = eval(f'F.{self.activision_function}(x)' )\n",
    "            x = self.batch_norm_1d(x)\n",
    "        else :\n",
    "            x = self.batch_norm_1d(x)\n",
    "            x = eval(f'F.{self.activision_function}(x)' )\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self,down_sample, n_filters_in, n_filters_out, \n",
    "                 dropout_keep_prob=0.8, kernel_size=17, preactivation=True,\n",
    "                 postactivation_bn=False, activation_function='relu', last_layer = 'sigmoid'):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "                \n",
    "        self.down_sample = down_sample    \n",
    "        self.n_filters_in = n_filters_in\n",
    "        self.n_filters_out = n_filters_out\n",
    "        self.dropout_rate = 1 - dropout_keep_prob\n",
    "        self.kernel_size = kernel_size\n",
    "        self.preactivation = preactivation\n",
    "        self.postactivation_bn = postactivation_bn\n",
    "        self.activation_function = activation_function\n",
    "        self.last_layer = last_layer\n",
    "        self.kernel_size = kernel_size\n",
    "      \n",
    "        \n",
    "        \n",
    "        self.conv_1d_1 = nn.Conv1d(self.n_filters_in, self.n_filters_out, kernel_size=self.kernel_size, padding = 8)\n",
    "        self.conv_1d_2 = nn.Conv1d(self.n_filters_out, self.n_filters_out, kernel_size=self.kernel_size, stride = self.down_sample,padding = 8)\n",
    "        self.skip_connection = SkipConnection(self.down_sample, self.n_filters_in,self.n_filters_out)\n",
    "        self.batch_norm_plus_activision = BatchNorm_PlusActivision(self.n_filters_out, self.activation_function, self.postactivation_bn)\n",
    "        self.dropout =  nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        y = self.skip_connection(y)\n",
    "        x = self.conv_1d_1(x)\n",
    "        x = self.conv_1d_2(x)\n",
    "        if self.dropout_rate > 0 :\n",
    "            x = self.dropout(x)\n",
    "        if self.preactivation:\n",
    "            x = torch.add(x,y)\n",
    "            y = x\n",
    "            x = self.batch_norm_plus_activision(x)\n",
    "            if self.dropout_rate > 0 :\n",
    "                 x = self.dropout(x)\n",
    "        else:\n",
    "            x = self.batch_norm_plus_activision(x)\n",
    "            x = torch.add(x,y)\n",
    "            x = eval(f'F.{self.activision_function}(x)' )\n",
    "            if self.dropout_rate > 0 :\n",
    "                    x = self.dropout(x)\n",
    "            y = x\n",
    "        return x,y\n",
    "    \n",
    "    \n",
    "class ResidualNetwork_classifier(nn.Module) :\n",
    "    def __init__(self, n_filters_in,\n",
    "                 dropout_keep_prob=0.8, kernel_size=17, preactivation=True,\n",
    "                 postactivation_bn=False, activation_function='relu', last_layer = 'sigmoid'):\n",
    "        super(ResidualNetwork_classifier, self).__init__()\n",
    "          \n",
    "        self.n_filters_in = n_filters_in\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.kernel_size = kernel_size\n",
    "        self.preactivation = preactivation\n",
    "        self.postactivation_bn = postactivation_bn\n",
    "        self.activation_function = activation_function\n",
    "        self.last_layer = last_layer\n",
    "        self.kernel_size = kernel_size\n",
    "      \n",
    "        \n",
    "        self.conv_1d = nn.Conv1d(n_filters_in,64, kernel_size=self.kernel_size, padding = 8)\n",
    "        self.batch_norm_1d = nn.BatchNorm1d(num_features = 64)\n",
    "        self.residual_unit_1 = ResidualUnit(4,64,128)\n",
    "        self.residual_unit_2 = ResidualUnit(4,128,196)\n",
    "        self.residual_unit_3 = ResidualUnit(4,196,256)\n",
    "        self.residual_unit_4 = ResidualUnit(4,256,320)\n",
    "        self.classifier = Classifier_Pool(30)\n",
    "        #self.linear = nn.Linear(5120, 9)\n",
    "        #self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        \n",
    "        x = self.conv_1d(x)\n",
    "        x = self.batch_norm_1d(x)\n",
    "        x = eval(f'F.{self.activation_function}(x)' ) \n",
    "        x,y = self.residual_unit_1(x,x)\n",
    "        x,y = self.residual_unit_2(x,y)\n",
    "        x,y = self.residual_unit_3(x,y)\n",
    "        x,_ = self.residual_unit_4(x,y)\n",
    "        #print(x.shape)\n",
    "        x,logit_maps = self.classifier(x)\n",
    "        \n",
    "        return x,logit_maps\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResidualNetwork_classifier(12)\n",
    "a= torch.rand((64,12,4096))\n",
    "x,y = model(a)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in range(len(y)):\n",
    "    temp = []\n",
    "    for j in range(len(y)):\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        x = cos(y[i],y[j])\n",
    "        c.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-33138a53790f>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-33138a53790f>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    for j in range(len(l):\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "l = torch.tensor([[1,0,0,0,1],[1,0,1,0,0],[0,0,1,1,0]])\n",
    "p = []\n",
    "for i in range(len(l)):\n",
    "    for j in range(len(l):\n",
    "        x = torch\n",
    "        p.append(x)\n",
    "q  = torch.Tensor(3,5,5)\n",
    "r = torch.cat(p, out=q)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[0].unsqueeze(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(l[0].unsqueeze(dim = 0).T,l[0].unsqueeze(dim = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in range(len(x)):\n",
    "    temp = []\n",
    "    for j in range(len(x)):\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        z = cos(x[i],x[j])\n",
    "        c.append(z)\n",
    "e = torch.stack(c, dim =0)\n",
    "e = e.reshape(30,30,128)\n",
    "e = torch.permute(e,(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = b\n",
    "p = []\n",
    "for i in range(len(l)):\n",
    "\n",
    "    expand = torch.mm(l[i].unsqueeze(dim = 1),l[i].unsqueeze(dim = 0))\n",
    "    p.append(expand)\n",
    "\n",
    "d = torch.stack(p, dim =0)\n",
    "n = d.shape[1]\n",
    "mask = torch.eye(n, n).bool()\n",
    "d = d[:].masked_fill_(mask, False)\n",
    "r = d*e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualNetwork_classifier(12)\n",
    "a,b,_ =next(itter("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [torch.rand(64,1,16),torch.rand(64,1,16) ,torch.rand(64,1,16) ,torch.rand(64,1,16) ,torch.rand(64,1,16) ] \n",
    "c = []\n",
    "\n",
    "for i in range(len(a)):\n",
    "    temp = []\n",
    "    for j in range(len(a)):\n",
    "        cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "        x = cos(a[i],a[j])\n",
    "        c.append(x)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "print(len(c))\n",
    "print(c[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.Tensor(25,64,1)\n",
    "d = torch.cat(c, out = b)\n",
    "d = d.reshape(5,5,64,1)\n",
    "d.squeeze().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.tensor([[1,0,0,0,1],[1,0,0,0,1],[1,0,0,0,1],[1,0,0,0,1]])\n",
    "p = []\n",
    "for i in range(len(l)):\n",
    "\n",
    "    expand = torch.mm(l[i].unsqueeze(dim = 1),l[i].unsqueeze(dim = 0))\n",
    "    p.append(expand)\n",
    "\n",
    "d = torch.stack(p, dim =0)\n",
    "n = d.shape[1]\n",
    "mask = torch.eye(n, n).bool()\n",
    "d[:].masked_fill_(mask, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'diag_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-02d52b054ea5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'diag_indices'"
     ]
    }
   ],
   "source": [
    "ind = torch.diag_indices(d[0].shape[0])\n",
    "d[:,ind[0],ind[1]] = 0\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~torch.tensor([1,0,0,0,1])+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.Tensor(25,64,1)\n",
    "d = torch.cat(c, out = b)\n",
    "d = d.reshape(5,5,64,1)\n",
    "d.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.eye(5, 5).byte()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 4096])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarityloss(logit_maps,labels) :\n",
    "    cosine_similarity_list = []\n",
    "    for i in range(len(logit_maps)):\n",
    "        for j in range(len(logit_maps)):\n",
    "            cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            c_similarity = cos(logit_maps[i],logit_maps[j])\n",
    "            #print(c_similarity.shape)\n",
    "            cosine_similarity_list.append(c_similarity)\n",
    "    cosine_similarity_tensor = torch.stack(cosine_similarity_list, dim =0)\n",
    "    cosine_similarity_tensor = cosine_similarity_tensor.reshape(30,30,128)\n",
    "    cosine_similarity_tensor = torch.permute(cosine_similarity_tensor,(2,0,1))\n",
    "    \n",
    "    ##Masking_tensor\n",
    "    \n",
    "    masking_list_cc = []\n",
    "    masking_list_ca = []\n",
    "    for i in range(len(labels)):\n",
    "        inverted_labels = ~labels[i].unsqueeze(dim = 0).int()+2\n",
    "        inverted_labels = inverted_labels.double()\n",
    "        expand_cc = torch.mm(labels[i].unsqueeze(dim = 1),labels[i].unsqueeze(dim = 0))\n",
    "        expand_ca = torch.mm(labels[i].unsqueeze(dim = 1),inverted_labels)\n",
    "        masking_list_cc.append(expand_cc)\n",
    "        masking_list_ca.append(expand_ca)\n",
    "\n",
    "    masking_tensor_cc = torch.stack(masking_list_cc, dim =0)\n",
    "    masking_tensor_ca = torch.stack(masking_list_ca, dim =0)\n",
    "    \n",
    "    n = masking_tensor_cc.shape[1]\n",
    "    mask = torch.eye(n, n).bool()\n",
    "    masking_tensor_cc = masking_tensor_cc[:].masked_fill_(mask, 0)\n",
    "    masking_tensor_ca = masking_tensor_ca[:].masked_fill_(mask, 0)\n",
    "    \n",
    "    loss = (.7*(masking_tensor_cc*cosine_similarity_tensor).sum()+.3*(masking_tensor_ca*cosine_similarity_tensor).sum())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = torch.rand(16,10,10)\n",
    "logit_maps = [a,a,a,a,a]\n",
    "\n",
    "\n",
    "n_class = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = cosine_similarityloss(logit_maps,None,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/999\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-251-e9b7bd836d55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader_files\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0minput_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1127\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1128\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-5470bb7249a2>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheader_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0morig_sr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#print(orig_sr)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\arrhythmia_detection_ECG\\helper_code.py\u001b[0m in \u001b[0;36mload_header\u001b[1;34m(header_file)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Load header file as a string.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[1;34m(do_setlocale)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"win\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "resume = None\n",
    "model = ResidualNetwork_classifier(12).cuda() \n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "Lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer , step_size = 5, gamma = .1)\n",
    "label_directory = 'test_data'\n",
    "output_directory = 'test_outputs'\n",
    "start_epoch = 0\n",
    "if resume:\n",
    "    if os.path.isfile(resume):\n",
    "      print_log(\"=> loading checkpoint '{}'\".format(resume), log)\n",
    "      checkpoint = torch.load(resume)\n",
    "      start_epoch = checkpoint['epoch']\n",
    "      Lr_scheduler_state_dict = checkpoint['Lr_scheduler_state_dict']\n",
    "      model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "      print_log(\"=> loaded checkpoint '{}' (epoch {})\" .format(resume, checkpoint['epoch']), log)\n",
    "    else:\n",
    "      print_log(\"=> no checkpoint found at '{}'\".format(resume), log)\n",
    "Lr_scheduler.step()\n",
    "for epoch in range(start_epoch,num_epochs):\n",
    "        since = time.time()\n",
    "        print('Epoch {}/{}'.format(epoch , num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "    \n",
    "        model.train() \n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, target, header_files) in tqdm.tqdm(enumerate(train_loader)):\n",
    "            \n",
    "            input_var = torch.autograd.Variable(inputs.cuda().float())\n",
    "            target_var = torch.autograd.Variable(target.cuda().float())\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            # loss\n",
    "            loss = criterion(output, target_var)\n",
    "#             print(output)\n",
    "#             print(target_var)\n",
    "        \n",
    "            if (i%5) == 0: \n",
    "                print('step: {} totalloss: {loss:.3f} '.format(i, loss = loss))\n",
    "                \n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "            \n",
    "\n",
    "            #print(loss.data.item()\n",
    "            \n",
    "            \n",
    "            \n",
    "            running_loss += loss.data.item()\n",
    "\n",
    "                \n",
    "            \n",
    "        Lr_scheduler.step()\n",
    "        epoch_loss = float(running_loss) / float(i)\n",
    "        print(' Epoch over  Loss: {:.5f}'.format(epoch_loss))\n",
    "        print('Testing_model............')\n",
    "        auroc,auc  = test_model(model, classes, test_loader, label_directory, output_directory)\n",
    "        PATH = f'model/Epoch-{epoch}-aur-{round(auroc,3)}-auc {round(auc,3)}.tar' \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            'Lr_scheduler_state_dict' :Lr_scheduler.state_dict()\n",
    "            }, PATH)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 5\n",
    "num_epochs = 10\n",
    "for epoch in range(start_epoch,num_epochs) :\n",
    "    print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " auroc,auc  = test_model(model, classes, test_loader, label_directory, output_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
